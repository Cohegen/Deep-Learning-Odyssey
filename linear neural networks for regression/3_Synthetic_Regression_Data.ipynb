{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj4ykK3V7xdy",
        "outputId": "e46b77a5-64bc-4f0e-aa72-a44dfc7cb9ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting d2l\n",
            "  Downloading d2l-1.0.3-py3-none-any.whl.metadata (556 bytes)\n",
            "Downloading d2l-1.0.3-py3-none-any.whl (111 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: d2l\n",
            "Successfully installed d2l-1.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install d2l --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import random\n",
        "import torch\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "10f0ysqR8DJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Generating a dataset."
      ],
      "metadata": {
        "id": "lfa-Vkgx8NB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here we will work in low dimension for accuracy.\n",
        "* Here we'll generate 1000 examples with 2-dimensional features drawn from a standard normal distribution.\n",
        "* Resulting to a matrix `X`.\n",
        "* We generate each label by applying a ground truth linear function, corrupting them via additive noise `e`, drawn independetly and identically for each example:\n",
        "\n",
        "  * `y=Xw + b +e`"
      ],
      "metadata": {
        "id": "eH2bPNs-8SQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SyntheticRegressionData(d2l.DataModule):\n",
        "  \"\"\"Synthetic data for linear regression\"\"\"\n",
        "  def __init__(self,w,b,noise=0.01,num_train=1000,num_val=1000,batch_size=32 ):\n",
        "    super().__init__()\n",
        "    self.save_hyperparameters()\n",
        "    n = num_train + num_val\n",
        "    self.X = torch.rand(n,len(w))\n",
        "    noise = torch.randn(n,1)*noise\n",
        "    self.y = torch.matmul(self.X,w.reshape((-1,1))) + b + noise\n"
      ],
      "metadata": {
        "id": "a6zH6jul9ItO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We set the true parameters $w=[2,-3.4]^T$ and $b=4.2$."
      ],
      "metadata": {
        "id": "U5Fx0I0N96hW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = SyntheticRegressionData(w=torch.tensor([2,-3.4]),b=4.2)"
      ],
      "metadata": {
        "id": "Z8cftqD4-IzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Each row in features consists of a vector in $R^2$ and each row in `labels` is a scalar."
      ],
      "metadata": {
        "id": "fSYswKT_-Xxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##looking at first entry\n",
        "print(f\"Features:{data.X[0]}\")\n",
        "print(f\"\\nLabel: {data.y[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6joSkj6q-llI",
        "outputId": "f24bcbba-fcd3-4f36-ae66-bf6e53213da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features:tensor([0.6352, 0.0926])\n",
            "\n",
            "Label: tensor([5.1511])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Reading the dataset.\n"
      ],
      "metadata": {
        "id": "I5I7tFNL-27z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Training ML models often requires multiple passes over the dataset, grabbing one minibatch of examples at a time and then the data is then used to update the model.\n",
        "* To illustrate how this works, we implement the `get_dataloader` method, registering it in the `SyntheticRegressionData` via `add_to_class`."
      ],
      "metadata": {
        "id": "oDanhp-J-_3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(SyntheticRegressionData)\n",
        "def get_dataloader(self,train):\n",
        "  if train:\n",
        "    indices = list(range(0,self.num_train))\n",
        "    #reading the examples in random order\n",
        "    random.shuffle(indices)\n",
        "\n",
        "  else:\n",
        "    indices = list(range(self.num_train,self.num_train+self.num_val))\n",
        "  for i in range(0,len(indices),self.batch_size):\n",
        "    batch_indices = torch.tensor(indices[i:i+self.batch_size])\n",
        "    yield self.X[batch_indices],self.y[batch_indices]"
      ],
      "metadata": {
        "id": "p2rcs_dA_jIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##inspecting the first minibatch\n",
        "X,y = next(iter(data.train_dataloader()))\n",
        "print('X.shape:',X.shape, '\\ny shape:',y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go0FL3MNAZjw",
        "outputId": "5f15c076-a99e-4f67-d78b-296bd58ab968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape: torch.Size([32, 2]) \n",
            "y shape: torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using iterators we've just built is inefficient in ways that might get us into trouble in the real world.\n",
        "* For example, it requires we load  all data in memory and that we perform lots of random memory access.\n",
        "* However,built-in iterators implemented in deep learning frameworks are considerably more efficient and they can deal with sources such as data stored in files, data received via a stream, and data generated or processed on the fly."
      ],
      "metadata": {
        "id": "OWL3BR0GBUbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Concise Implementation of the DataLoader"
      ],
      "metadata": {
        "id": "LycLIbEUA4FZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Rather than writing our own iterator, we can call the existing API framework to load data.\n",
        "* We set `batch_size` in the built-in data loader and let it take care of shufflong examples efficiently."
      ],
      "metadata": {
        "id": "urGOuluCA82K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.add_to_class(d2l.DataModule)\n",
        "def get_tensorloader(self,tensors,train,indices=slice(0,None)):\n",
        "  tensors = tuple(a[indices]for a in tensors)\n",
        "  dataset = torch.utils.data.TensorDataset(*tensors)\n",
        "  return torch.utils.data.DataLoader(dataset,self.batch_size,shuffle=train)\n",
        "\n",
        "d2l.add_to_class(SyntheticRegressionData)\n",
        "def get_dataloader(self,train):\n",
        "  i = slice(0,self.num_train)if train else slice(self.num_train,None)\n",
        "  return self.get_tensorloader((self.X,self.y),train,i)"
      ],
      "metadata": {
        "id": "Dr5EnG8rB_N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The new dataloader behaves just like the previous one, except that it more efficient and has some added functionality."
      ],
      "metadata": {
        "id": "L3q3RHTMDOwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = next(iter(data.train_dataloader()))\n",
        "print('X.shape',X.shape,'\\ny shape: ',y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deiAGw3bDYOA",
        "outputId": "a05ac03c-8617-4a9c-a349-ba1c28c32b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape torch.Size([32, 2]) \n",
            "y shape:  torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The dataloader supports the `__len__` method, which we can use to query the length of dataloders."
      ],
      "metadata": {
        "id": "w7VD-iCXDmXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atzOpwwrDwpc",
        "outputId": "598f8a00-42fb-4f56-d85d-686f4a72a207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##implementing a data generator that produces new data on the fly,\n",
        "##every time the iterator is called\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def data_generator(w,b,batch_size):\n",
        "  \"\"\"Generates synthetic data on the fly\"\"\"\n",
        "  num_features = len(w)\n",
        "\n",
        "  while True:\n",
        "    #generate random features (x) for the batch\n",
        "    X = torch.randn(batch_size,num_features)\n",
        "\n",
        "    #calculating labels\n",
        "    y = torch.matmul(X,w) + b\n",
        "\n",
        "    #adding gaussina noise\n",
        "    y += torch.randn(y.shape)*0.01\n",
        "\n",
        "    #yield function pauses the function and returns the batch\n",
        "    #when the iterator is called again, it resumes\n",
        "    yield X,y.reshape((-1,1))\n",
        "\n",
        "true_w = torch.tensor([2,-3.5,0.5])\n",
        "true_b = 4.2\n",
        "batch_size = 10\n",
        "\n",
        "data_iter = data_generator(true_w,true_b,batch_size)\n",
        "\n",
        "#getting the first \"on-the-fly\" batch\n",
        "X_batch,y_batch = next(data_iter)\n",
        "\n",
        "print(\"First batch of features (X):\\n\", X_batch)\n",
        "print(\"\\nFirst batch of labels (y):\\n\", y_batch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnI7NAvVF6ia",
        "outputId": "25254d2c-b454-4ea8-a52d-6bb2dcedf9e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First batch of features (X):\n",
            " tensor([[ 0.5723,  1.3739, -1.8686],\n",
            "        [ 0.3060,  0.9130,  0.7138],\n",
            "        [ 1.3909,  0.7003,  0.8485],\n",
            "        [ 0.4367,  0.6296, -0.5597],\n",
            "        [ 0.2494, -1.1324, -1.1059],\n",
            "        [-0.0696,  0.5318,  0.6042],\n",
            "        [ 1.8589, -0.7398,  0.9072],\n",
            "        [ 0.7863,  0.2056,  0.7796],\n",
            "        [ 0.3892,  0.2855, -1.7331],\n",
            "        [-0.3145,  0.1259,  1.6988]])\n",
            "\n",
            "First batch of labels (y):\n",
            " tensor([[-0.3917],\n",
            "        [ 1.9866],\n",
            "        [ 4.9611],\n",
            "        [ 2.5830],\n",
            "        [ 8.1056],\n",
            "        [ 2.5204],\n",
            "        [10.9670],\n",
            "        [ 5.4257],\n",
            "        [ 3.1212],\n",
            "        [ 3.9784]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can see from above that given a dataset of examples we reshuffle it but why do we need to reshuffle the data points in the dataset?\n",
        "1. Preventing `shortcut learning`- if data is ordered the model finds the \"cheat code\". For example if we have a dataset with cat and dogs images. And say we divide the dataset into two batches one containing dog and cat images. As the model sees the cat images it may see that since cat are indoors with stuff like carpets so that means that's an image of a cat.\n",
        "If data is shuffled this cheat code it broken. The only way the model can reduce loss consistently is bylearning the actual features shape,color that define the object.\n",
        "2. Smooting the gradient path:\n",
        "in unordered data,each training step is based on a random sample, so we can generally move towards the bottom.\n",
        "In ordered we will take like 100 steps to the left 100 steps to the right and never end at the bottom.\n"
      ],
      "metadata": {
        "id": "6aaNLbzZWrc-"
      }
    }
  ]
}