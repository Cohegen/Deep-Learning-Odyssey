{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* Regression problems pop up whenever we want to predict anumerical value.\n",
        "* Such cases include predicting prices,predicting the length of stay for patient in say a hospital, forecastinf demand etc.\n",
        "* To develop a model for predicting house prices, we need to get our hands on data, which includes sales price,area,number of rooms etc.\n",
        "* This dataset is called a `training dataset` or `training set`, and each row that contains data corrresponding to one sale is called a data point or sample.\n",
        "* The thing that we're trying to predict is called a *label* or *target*.\n",
        "* The variables upon which the predictions are based are called `features or covariates`"
      ],
      "metadata": {
        "id": "Z_GCOgVzMmjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l --no-deps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFwVtKdQOGu5",
        "outputId": "28117561-7660-4f1e-903d-129b0af95a9e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: d2l in /usr/local/lib/python3.12/dist-packages (1.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "3Y5DqRy8NwBa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Basics"
      ],
      "metadata": {
        "id": "Gc1YrbL9PZvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In linear regression we first assume that the relationship between features `x` and target `y` is approximately linear.\n",
        "* We superscrpits to enumerate samples and targets, subscripts to index coordinates.\n",
        "* $x^{(i)}$ denotes the $i^{th}$ and $x_j^{(i)}$ denotes its $j^{th}$  coordinate."
      ],
      "metadata": {
        "id": "A8P8vzYuPb92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "gMA6sUbZQq4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The assumption of linearity means that the expected value of the target (price) can be expressed as a weighted sum of the features (area and age):\n",
        "\n",
        "* $price = w_{area}* area + w_{age}*age + b$\n",
        "\n",
        "* Here $w_{area}$ and   $w_{age} $ are called *weights* and $b$ is called $bias$.\n",
        "* The weights determine the influence of each feature on our prediction.\n",
        "* The bias determines the value of the estimate when all features are zero.\n",
        "* In machine learning, we usually work with high-dimensional datasets, where it is more convenient to employ compact linear algebra notation.\n",
        "* When our inputs consists of $d$ features, we assign each an index (between 0 and d) and express our prediction $\\hat{y}$ as:\n",
        "   \n",
        "   $\\hat{y}$ = $w_1*x_1 + ...+ w_d*x_d + b $\n",
        "\n",
        "* We can also express our model compactly via dot product between `w` and `x`.\n",
        "\n",
        "  $\\hat{y}$ = $w^{T}*x + b$\n",
        "\n",
        "* The vector `x` in the above formula corresponds to the features of a single examples"
      ],
      "metadata": {
        "id": "04oYiucdQva9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##implementing a basic regression example\n",
        "import numpy as np\n",
        "x = np.arange(12)\n",
        "y_true = np.arange(12,24)\n",
        "b = 1.5\n",
        "y_hat = []\n",
        "\n",
        "# For a single-feature linear regression, 'w' should be a single scalar weight.\n",
        "w = 1.0\n",
        "\n",
        "for x_val in x: # Iterate through each feature value in x\n",
        "  preds = w * x_val + b\n",
        "  y_hat.append(preds)\n",
        "\n",
        "print(y_hat)\n",
        "print(f\"Number of predictions: {len(y_hat)}\")\n",
        "print(f\"True values: {y_true}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Ph-uKebnd1",
        "outputId": "fd6a133b-81d0-44d6-9082-3ba4b35213ec"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[np.float64(1.5), np.float64(2.5), np.float64(3.5), np.float64(4.5), np.float64(5.5), np.float64(6.5), np.float64(7.5), np.float64(8.5), np.float64(9.5), np.float64(10.5), np.float64(11.5), np.float64(12.5)]\n",
            "Number of predictions: 12\n",
            "True values: [12 13 14 15 16 17 18 19 20 21 22 23]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function"
      ],
      "metadata": {
        "id": "9Ov7LjqdUOTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Say we want to measure how accurate our model performs like how good is it's performance.\n",
        "* We use a function called a *loss function* to do this.\n",
        "* Basically it quantifies the distance between `real` and `predicted` values of the target.\n",
        "* For regression problems, the most common loss function is the $ squared error$.\n",
        "* When our prediction for an example $i$ is $\\hat{y}^{(i)}$ and corresponding true label is $y^{(i)}$, the squared error is given by:\n",
        "\n",
        "$l^{(i)}(w, b) = \\frac{1}{2} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2$\n",
        "\n",
        "* The constant $1/2$ makes no real differencw but proves to be notatioanally convenient, since it cancels out when we take the derivative of the loss.\n",
        "\n",
        "* To measure the quality of a model on the entire dataset of `n` samples we simply average the losses on the training set:\n",
        "\n",
        " $\n",
        " L(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^{n} l^{(i)}(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2} \\left( \\mathbf{w}x^{(i)} + b - y^{(i)} \\right)^2\n",
        " $\n",
        "\n",
        " * When training the model, we seek parameter $(w*,b*)$ that minimizes the total loss across all training examples:\n",
        " $\\mathbf{w}^*, b^* = \\underset{\\mathbf{w}, b}{\\mathrm{argmin}} \\ L(\\mathbf{w}, b)$\n",
        "\n"
      ],
      "metadata": {
        "id": "TcoXAga3UQDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##defining loss function\n",
        "# Convert y_hat to a numpy array for element-wise operations\n",
        "y_hat_np = np.array(y_hat)\n",
        "\n",
        "# Calculate the squared error for each prediction and then take the mean\n",
        "# according to the formula L(w, b) = (1/n) * sum( (y_hat - y_true)^2 )\n",
        "# The 0.5 factor is often included in the definition of MSE for convenience in differentiation.\n",
        "loss = 0.5 * np.mean((y_hat_np - y_true)**2)\n",
        "\n",
        "print(f\"Average loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8WyMYVwe8VT",
        "outputId": "d93caf53-34fc-4e04-d472-34cb4358d29d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average loss: 55.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analytic Solution"
      ],
      "metadata": {
        "id": "_IYX36PyYjpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can find the optimal parameters analytically by applyinf a simple formula as folows.\n",
        "* First, we can subsume that bias `b` into the parameter `w` by appending a column to the design matrix consists of all 1s. Then our prediction problem is to minimize $|| y -Xw||^2$.\n",
        "* As long as the matrix X has full rank that is no feature is linearly dependent on the others, then there wil be just one critical point on the loss surface and it corresponds to the minimum of the loss over the entire domain.\n",
        "* Taking the derivative of the loss with respect to `w` and setting it equal to zero yields:\n",
        " $\\frac{\\partial}{\\partial \\mathbf{w}} \\|\\mathbf{y} - \\mathbf{Xw}\\|^2 = 2\\mathbf{X}^\\top (\\mathbf{Xw} - \\mathbf{y}) = 0$ hence $X^Ty = X^TXw$."
      ],
      "metadata": {
        "id": "svakOYn0YnRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minibatch Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "3hD5tGjybbwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The key technique for optimizing nearly every deep learning model, consists of iteratively reducing the error by updating the parameters in the direction that incrementally lowers the loss function.\n",
        "\n",
        "* This algorithm is called `gradient descent`.\n",
        "* Think of it as a hiker trying to find the lowest point in a mountaineous valley, while being sorrounded by fog, they can't see the bottom but they can feel the sloe of ground under their feet and take a step downhill.\n"
      ],
      "metadata": {
        "id": "7jHHPHiRg-P6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f7d86d7"
      },
      "source": [
        "## 1.2 Training the Model: Stochastic Gradient Descent\n",
        "\n",
        "Now that we have our model and loss function, we need a way to find the optimal values for `w` and `b` that minimize the loss. We'll use a simplified version of Stochastic Gradient Descent (SGD) for this demonstration. In practice, we'd use mini-batch SGD as discussed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84908317",
        "outputId": "98d7c331-6325-469c-dca7-f0f47279d474"
      },
      "source": [
        "# Initialize parameters\n",
        "w = np.array([0.0]) # Start with some initial weight\n",
        "b = 0.0              # Start with some initial bias\n",
        "\n",
        "# Learning rate (controls the step size in each update)\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Number of epochs (how many times to iterate over the dataset)\n",
        "num_epochs = 100\n",
        "\n",
        "# Store loss values to visualize training progress\n",
        "losses = []\n",
        "\n",
        "print(f\"Initial w: {w[0]:.4f}, Initial b: {b:.4f}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Calculate predictions\n",
        "    y_hat_epoch = w[0] * x + b\n",
        "\n",
        "    # Calculate the gradient of the loss with respect to w and b\n",
        "    # Loss: L(w, b) = 0.5 * (1/n) * sum((y_hat - y_true)^2)\n",
        "    # dL/dw = (1/n) * sum((y_hat - y_true) * x)\n",
        "    # dL/db = (1/n) * sum(y_hat - y_true)\n",
        "\n",
        "    grad_w = np.mean((y_hat_epoch - y_true) * x)\n",
        "    grad_b = np.mean(y_hat_epoch - y_true)\n",
        "\n",
        "    # Update parameters using gradient descent\n",
        "    w[0] = w[0] - learning_rate * grad_w\n",
        "    b = b - learning_rate * grad_b\n",
        "\n",
        "    # Calculate and store the loss for this epoch\n",
        "    current_loss = 0.5 * np.mean((y_hat_epoch - y_true)**2)\n",
        "    losses.append(current_loss)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch + 1:3d}, Loss: {current_loss:.4f}, w: {w[0]:.4f}, b: {b:.4f}\")\n",
        "\n",
        "print(f\"\\nFinal w: {w[0]:.4f}, Final b: {b:.4f}\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial w: 0.0000, Initial b: 0.0000\n",
            "Epoch  10, Loss: 18.3042, w: 2.4806, b: 0.6514\n",
            "Epoch  20, Loss: 17.3080, w: 2.4490, b: 0.9640\n",
            "Epoch  30, Loss: 16.3711, w: 2.4093, b: 1.2668\n",
            "Epoch  40, Loss: 15.4850, w: 2.3706, b: 1.5613\n",
            "Epoch  50, Loss: 14.6468, w: 2.3330, b: 1.8478\n",
            "Epoch  60, Loss: 13.8540, w: 2.2964, b: 2.1263\n",
            "Epoch  70, Loss: 13.1042, w: 2.2608, b: 2.3973\n",
            "Epoch  80, Loss: 12.3949, w: 2.2262, b: 2.6608\n",
            "Epoch  90, Loss: 11.7240, w: 2.1926, b: 2.9171\n",
            "Epoch 100, Loss: 11.0894, w: 2.1599, b: 3.1663\n",
            "\n",
            "Final w: 2.1599, Final b: 3.1663\n"
          ]
        }
      ]
    }
  ]
}