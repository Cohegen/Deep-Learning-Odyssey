{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* In machine learning our goal is to discover `patterns`.\n",
        "* But how can we be sure we have truly discovered a `general` pattern and not simply memorized our data?\n",
        "* This problem of search for a way to discover patterns within our data is  called `generalization`.\n",
        "* Whenever we work with finite samples of data,we must keep in mind the risk that we might fit out training data, only to discover we have failed to discover a generalizable pattern.\n",
        "* The phenomenon of fitting closer to our training data than underlying distribution is called `overfitting`, and the techniques of combatting  overfitting is often called `regularization` methods."
      ],
      "metadata": {
        "id": "_dBlRLN3CpYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Training Error and Generalization Error."
      ],
      "metadata": {
        "id": "H-SNmgQXEAvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Training error $R_{emp}$ is a statistic calculated on the training datasets while the generalization error $R$ is an expectation taken with respect to the underlying distribution.\n",
        "* The generalization error can be thought of as what you would see if you applied your model to an infinite stream of additional data examples drawn from the same underlying data distribution.\n",
        "* The training error is expressed as a $sum$:\n",
        "$R_{emp}(f) = \\frac{1}{n} \\sum_{i=1}^{n} L(y^{(i)}, f(x^{(i)}))$\n",
        "\n",
        "* where:\n",
        "    1. $n$ is the total number of samples in your training dataset.\n",
        "    2. $x^{(i)}$ - the input features for the $i$-th observation.\n",
        "    3. $y^{(i)}$ - the true label or target value for the $i$-th observation.\n",
        "    4. $f(x^{(i)})$ - the model's prediction for the $i$-th observation.\n",
        "    5. $L$- The Loss Function, which calculates the difference between the prediction and truth.\n",
        "\n",
        "* While the generalization error is expressed as an integral:\n",
        "$R[p, f] = \\mathbb{E}_{(x,y) \\sim P} [l(x, y, f(x))] = \\iint l(x, y, f(x)) p(x, y) \\, dx \\, dy$\n",
        "   \n",
        "* A problem arises since we cannot calculate the generalization $R$ exactly.\n",
        "* Since nobody ever tells us the precise form the density function $p(x,y)$.\n",
        "* Moreover we cannot sample an infinite stream of data points.\n",
        "* Thus in practice, we must `estimate` the generalization error by applying our model to an independent test set consituted of a random slection of examples `X` and labels `y^` that were witheld from our training set.\n",
        "* This consists of applying the same formula that was used  for calculating the empirical training error but to the test set `X,y^`."
      ],
      "metadata": {
        "id": "ffxeuVN_EUOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2 Model Complexity."
      ],
      "metadata": {
        "id": "LUne9SjLIlNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  With `simple models` and `abundant data`, training error and generalization error are usually close.\n",
        "* As model complexity increases or when data becomes scarce, the training error error may decrease while the generalization gap grows. This is because as complex model has more parameters and more flexible forms. Meaning it can memorize the little training examples and adjust parameters to match each example closely or perfectly. The generalization gap grows because the model memorizes the small training set, achieving low training error,but when it sees new data, its test error increases.\n",
        "* Extremely expressively models can perfectly fit even random labels making training error alone meaningless for judging generalization.\n",
        "* Without restrictions on model complexity, fitting training data does not guarantee that a model has learned a generalizable pattern.\n",
        "* Learning theory draws inspiration from Karl Popper's falsibility principle: a useful scientific theory must rule out some possibilities, not explain everything.\n",
        "* Model complexity is not determined only by the  number of parameters:\n",
        "  1. Some models e.g kernel methods have infinitely many parameters but are controlled by other constraints.\n",
        "  2. One useful notion of complexity is the range of allowed parameter values motivating techniques like regularization e.g weight decay.\n",
        "* However, comparing complexity across very different model classes i.e decision trees,neural networks is often difficult.\n",
        "* Low training error does not imply low generalixation error especially for high expressive models. This is because highly expressive models can fit the training data in many fundementally different ways, most of which capture noise (erronous data points) rather than true structure,and therefore training error cannot tell which one was chosen.\n",
        "* However, low training error also does not necessarily imply poor generalization.\n",
        "* For powerful models like deep neural networks, generalization must be assessed by using holdout data.\n",
        "* The error measured on this holdout (validation) set is called validation error."
      ],
      "metadata": {
        "id": "TGCyX6MKKTFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Underfitting or Overfitting."
      ],
      "metadata": {
        "id": "g5zh4RKzQrE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* When comparing training and valiidation errors, two common situatons are important to recognize.\n",
        "* The first one is a phenomenon known as `underfitting` which occurs when both the training and validation erors are high and are close to each other. Here the model is too simple to capture the underlying pattern. There's also a generalization gap suggesting the model benefits from increased complexity.\n",
        "\n",
        "* Second `overfitting` occurs when training error is much lower than the validation error. Here the model fits the training data well but does not generalize effectively.\n",
        "* The main objective is to try to minimize the generalizatio error, not necessarily the gap itself.\n",
        "* However, if training error reaches zero, the generalization gap equal the generalization error, and further immprovement is possible only by reducing this gap."
      ],
      "metadata": {
        "id": "xx_iofyjREZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Polynomial Curve Fitting\n",
        "\n",
        "* To illustrate some intuition about overfitting and model complexity, we consider the following: given training data consisting of a single feature `x` and a corresponding real-valued label `y`, we try to find the polynomial of degree `d`.\n",
        "$\n",
        "  \\hat{y} = \\sum_{i=0}^{d} w_i x^i\n",
        "  $\n",
        "for estimating the label `y`.\n",
        "* This is just a linear regression problem where our features are given powers of `x`, the models weights are given by $w_i$ and the bias is given by $w_0$ since $x^{(0)} = 1 $ for all $x$.\n",
        "* Higher degree polynomials are more complex because: they have more parameters and can represent wider range of functions.\n",
        "* For a fixed training dataset, increasing the polynomial degree can only decrease or maintain training error.\n",
        "* If all training inputs `x` are distinct, a polynomial with degree equal to the number of data points can fit the training data perfectly.\n"
      ],
      "metadata": {
        "id": "K4_d4GhaTWfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.4 Model Selection"
      ],
      "metadata": {
        "id": "BVUlqleZWIjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Typically we select our final model only after evaluating multiple models that differ in various ways in terms of architectures,training objectives,selected features etc.\n",
        "*  Choosing among many models is called `Model selection`."
      ],
      "metadata": {
        "id": "pjE3bOQwWaL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-Validation"
      ],
      "metadata": {
        "id": "yOvf40PyZncQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Wehn training data is scarce, we might not even be able to afford to validate enough data to consitute a proper validation set.\n",
        "* One popular solution is to employ `K-fold cross validation`.\n",
        "* Here, the original training data is split into $K$ non-overlapping subseys.\n",
        "* Then the model training and validation are executed $K$ times, each time training on $K-1$ subsets and validating on a different subset i.e the one not used for training in that round.\n",
        "* Finally, the training and validation erros are estimated by averaging over results from the `K` experiments.\n",
        "\n",
        "\n",
        "* K-fold cross-validation can be computationally expensive because the model must be trained and validated **K separate times**. When working with **large datasets** and **complex models**, each training and validation cycle requires significant computational resources, making the overall process costly in terms of time and computation.\n"
      ],
      "metadata": {
        "id": "GRO5o8F6ZrVi"
      }
    }
  ]
}