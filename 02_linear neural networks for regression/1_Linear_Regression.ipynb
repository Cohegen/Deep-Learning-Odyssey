{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* Regression problems pop up whenever we want to predict anumerical value.\n",
        "* Such cases include predicting prices,predicting the length of stay for patient in say a hospital, forecastinf demand etc.\n",
        "* To develop a model for predicting house prices, we need to get our hands on data, which includes sales price,area,number of rooms etc.\n",
        "* This dataset is called a `training dataset` or `training set`, and each row that contains data corrresponding to one sale is called a data point or sample.\n",
        "* The thing that we're trying to predict is called a *label* or *target*.\n",
        "* The variables upon which the predictions are based are called `features or covariates`"
      ],
      "metadata": {
        "id": "Z_GCOgVzMmjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l --no-deps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFwVtKdQOGu5",
        "outputId": "a0160b8d-b545-4689-edcb-cf59cff29757"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting d2l\n",
            "  Downloading d2l-1.0.3-py3-none-any.whl.metadata (556 bytes)\n",
            "Downloading d2l-1.0.3-py3-none-any.whl (111 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: d2l\n",
            "Successfully installed d2l-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "3Y5DqRy8NwBa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Basics"
      ],
      "metadata": {
        "id": "Gc1YrbL9PZvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In linear regression we first assume that the relationship between features `x` and target `y` is approximately linear.\n",
        "* We superscrpits to enumerate samples and targets, subscripts to index coordinates.\n",
        "* $x^{(i)}$ denotes the $i^{th}$ and $x_j^{(i)}$ denotes its $j^{th}$  coordinate."
      ],
      "metadata": {
        "id": "A8P8vzYuPb92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "gMA6sUbZQq4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The assumption of linearity means that the expected value of the target (price) can be expressed as a weighted sum of the features (area and age):\n",
        "\n",
        "* $price = w_{area}* area + w_{age}*age + b$\n",
        "\n",
        "* Here $w_{area}$ and   $w_{age} $ are called *weights* and $b$ is called $bias$.\n",
        "* The weights determine the influence of each feature on our prediction.\n",
        "* The bias determines the value of the estimate when all features are zero.\n",
        "* In machine learning, we usually work with high-dimensional datasets, where it is more convenient to employ compact linear algebra notation.\n",
        "* When our inputs consists of $d$ features, we assign each an index (between 0 and d) and express our prediction $\\hat{y}$ as:\n",
        "   \n",
        "   $\\hat{y}$ = $w_1*x_1 + ...+ w_d*x_d + b $\n",
        "\n",
        "* We can also express our model compactly via dot product between `w` and `x`.\n",
        "\n",
        "  $\\hat{y}$ = $w^{T}*x + b$\n",
        "\n",
        "* The vector `x` in the above formula corresponds to the features of a single examples"
      ],
      "metadata": {
        "id": "04oYiucdQva9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##implementing a basic regression example\n",
        "import numpy as np\n",
        "x = np.arange(12)\n",
        "y_true = np.arange(12,24)\n",
        "b = 1.5\n",
        "y_hat = []\n",
        "\n",
        "# For a single-feature linear regression, 'w' should be a single scalar weight.\n",
        "w = 1.0\n",
        "\n",
        "for x_val in x: # Iterate through each feature value in x\n",
        "  preds = w * x_val + b\n",
        "  y_hat.append(preds)\n",
        "\n",
        "print(y_hat)\n",
        "print(f\"Number of predictions: {len(y_hat)}\")\n",
        "print(f\"True values: {y_true}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Ph-uKebnd1",
        "outputId": "85ea121a-ad1f-4194-8ac6-328e75357661"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[np.float64(1.5), np.float64(2.5), np.float64(3.5), np.float64(4.5), np.float64(5.5), np.float64(6.5), np.float64(7.5), np.float64(8.5), np.float64(9.5), np.float64(10.5), np.float64(11.5), np.float64(12.5)]\n",
            "Number of predictions: 12\n",
            "True values: [12 13 14 15 16 17 18 19 20 21 22 23]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function"
      ],
      "metadata": {
        "id": "9Ov7LjqdUOTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Say we want to measure how accurate our model performs like how good is it's performance.\n",
        "* We use a function called a *loss function* to do this.\n",
        "* Basically it quantifies the distance between `real` and `predicted` values of the target.\n",
        "* For regression problems, the most common loss function is the $ squared error$.\n",
        "* When our prediction for an example $i$ is $\\hat{y}^{(i)}$ and corresponding true label is $y^{(i)}$, the squared error is given by:\n",
        "\n",
        "$l^{(i)}(w, b) = \\frac{1}{2} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2$\n",
        "\n",
        "* The constant $1/2$ makes no real differencw but proves to be notatioanally convenient, since it cancels out when we take the derivative of the loss.\n",
        "\n",
        "* To measure the quality of a model on the entire dataset of `n` samples we simply average the losses on the training set:\n",
        "\n",
        " $\n",
        " L(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^{n} l^{(i)}(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2} \\left( \\mathbf{w}x^{(i)} + b - y^{(i)} \\right)^2\n",
        " $\n",
        "\n",
        " * When training the model, we seek parameter $(w*,b*)$ that minimizes the total loss across all training examples:\n",
        " $\\mathbf{w}^*, b^* = \\underset{\\mathbf{w}, b}{\\mathrm{argmin}} \\ L(\\mathbf{w}, b)$\n",
        "\n"
      ],
      "metadata": {
        "id": "TcoXAga3UQDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##defining loss function\n",
        "# Convert y_hat to a numpy array for element-wise operations\n",
        "y_hat_np = np.array(y_hat)\n",
        "\n",
        "# Calculate the squared error for each prediction and then take the mean\n",
        "# according to the formula L(w, b) = (1/n) * sum( (y_hat - y_true)^2 )\n",
        "# The 0.5 factor is often included in the definition of MSE for convenience in differentiation.\n",
        "loss = 0.5 * np.mean((y_hat_np - y_true)**2)\n",
        "\n",
        "print(f\"Average loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8WyMYVwe8VT",
        "outputId": "6238da11-e792-4521-8d39-7e178a5e0ef6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average loss: 55.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analytic Solution"
      ],
      "metadata": {
        "id": "_IYX36PyYjpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can find the optimal parameters analytically by applyinf a simple formula as folows.\n",
        "* First, we can subsume that bias `b` into the parameter `w` by appending a column to the design matrix consists of all 1s. Then our prediction problem is to minimize $|| y -Xw||^2$.\n",
        "* As long as the matrix X has full rank that is no feature is linearly dependent on the others, then there wil be just one critical point on the loss surface and it corresponds to the minimum of the loss over the entire domain.\n",
        "* Taking the derivative of the loss with respect to `w` and setting it equal to zero yields:\n",
        " $\\frac{\\partial}{\\partial \\mathbf{w}} \\|\\mathbf{y} - \\mathbf{Xw}\\|^2 = 2\\mathbf{X}^\\top (\\mathbf{Xw} - \\mathbf{y}) = 0$ hence $X^Ty = X^TXw$."
      ],
      "metadata": {
        "id": "svakOYn0YnRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minibatch Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "3hD5tGjybbwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The key technique for optimizing nearly every deep learning model, consists of iteratively reducing the error by updating the parameters in the direction that incrementally lowers the loss function.\n",
        "\n",
        "* This algorithm is called `gradient descent`.\n",
        "* Think of it as a hiker trying to find the lowest point in a mountaineous valley, while being sorrounded by fog, they can't see the bottom but they can feel the sloe of ground under their feet and take a step downhill.\n"
      ],
      "metadata": {
        "id": "7jHHPHiRg-P6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variants of gradients Descent"
      ],
      "metadata": {
        "id": "N3uIsqR89367"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Stochastic Gradient Descent: Here the SGD  algorithm looks at only one random training example, calculates loss, and updates parameters immediately.\n",
        "* Cons:\n",
        "  * 1. It is noisy and has unstable updates - SGD uses one samle at a time. This makes the gradient estimate noisy(updates are incosistent and fluctuate a lot) which results to making convergence unsmooth.\n",
        "  * 2. Slow convergence at exact minimum- near the optimum,SGD keeps oscillating instead of settling which in turn makes it never converge, and only hover around the minimum.\n",
        "  * 3. It's highly sensitive to learning rate i.e if the learning rate is too large it divergence away from the optimal minimum, if it's too small it will have an extremely slow learning.\n",
        "\n",
        "* 2. Minibatch Stochastic Gradient Descent- here instead of using one example or the whole dataset,it uses small groups of examples (Minibatchs) typically between 32 and 512.\n",
        "  * Advantages:\n",
        "     1. It is less noisy than SGD since it uses small batches instead of one sample hence making gradient estimates more stable which in turn makes the loss curve smoother.\n",
        "     2. It is faster than batch gradient descent since parameters are updated multiple times per epoch.\n",
        "    3. It ensures efficient use of hardware i.e TPU/GPUs since vectorized operations on minibatchs are highly optimized\n",
        "* Let's implement a code version of minibatch gradient descent."
      ],
      "metadata": {
        "id": "rTjtrtsN9-q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.arange(1,9)\n",
        "y_true = 2*x\n",
        "\n",
        "# Initialize parameters as floats for accurate updates\n",
        "weight = 2.0\n",
        "bias = 1.0\n",
        "learning_rate = 0.01\n",
        "minibatch_size = 4\n",
        "epochs = 100\n",
        "\n",
        "# Prepare data batches\n",
        "x_batches = np.array_split(x, len(x) // minibatch_size)\n",
        "y_true_batches = np.array_split(y_true, len(y_true) // minibatch_size)\n",
        "\n",
        "print(f\"Initial Weight: {weight:.4f}, Bias: {bias:.4f}\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "  total_epoch_loss = 0.0\n",
        "  for x_batch, y_batch in zip(x_batches, y_true_batches):\n",
        "    # 1. Make prediction with current weights and bias\n",
        "    y_hat_batch = weight * x_batch + bias\n",
        "\n",
        "    # Calculate the error term\n",
        "    error = y_hat_batch - y_batch\n",
        "\n",
        "    # 2. Calculate batch loss (Mean Squared Error)\n",
        "    batch_loss = np.mean(0.5 * (error**2)) # Using 0.5 * squared error as defined\n",
        "    total_epoch_loss += batch_loss\n",
        "\n",
        "    # 3. Calculate gradients (mean of the derivatives for the batch)\n",
        "    # As per the loss function L = (1/n) * sum(0.5 * (y_hat - y)^2)\n",
        "    # dL/dw = (1/n) * sum((y_hat - y) * x)\n",
        "    # dL/db = (1/n) * sum(y_hat - y)\n",
        "    grad_w = np.mean(error * x_batch)\n",
        "    grad_b = np.mean(error)\n",
        "\n",
        "    # 4. Update parameters\n",
        "    weight = weight - learning_rate * grad_w\n",
        "    bias = bias - learning_rate * grad_b\n",
        "\n",
        "  # Print average loss for the epoch to monitor progress\n",
        "  if (epoch + 1) % 10 == 0 or epoch == 1: # Prints every 10 epochs or first epoch\n",
        "      avg_epoch_loss = total_epoch_loss / len(x_batches)\n",
        "      print(f\"Epoch {epoch + 1}: Loss = {avg_epoch_loss:.4f}, Weight = {weight:.4f}, Bias = {bias:.4f}\")\n",
        "\n",
        "print(f\"\\nFinal Weight: {weight:.4f}, Bias: {bias:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1HIWktWCQvW",
        "outputId": "60d6b560-8b34-46da-a991-c561c100d7d0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weight: 2.0000, Bias: 1.0000\n",
            "Epoch 2: Loss = 0.1885, Weight = 1.8818, Bias = 0.9704\n",
            "Epoch 10: Loss = 0.0972, Weight = 1.8452, Bias = 0.9314\n",
            "Epoch 20: Loss = 0.0894, Weight = 1.8513, Bias = 0.8934\n",
            "Epoch 30: Loss = 0.0822, Weight = 1.8574, Bias = 0.8570\n",
            "Epoch 40: Loss = 0.0757, Weight = 1.8632, Bias = 0.8221\n",
            "Epoch 50: Loss = 0.0696, Weight = 1.8688, Bias = 0.7886\n",
            "Epoch 60: Loss = 0.0641, Weight = 1.8741, Bias = 0.7564\n",
            "Epoch 70: Loss = 0.0590, Weight = 1.8793, Bias = 0.7256\n",
            "Epoch 80: Loss = 0.0543, Weight = 1.8842, Bias = 0.6961\n",
            "Epoch 90: Loss = 0.0499, Weight = 1.8889, Bias = 0.6677\n",
            "Epoch 100: Loss = 0.0459, Weight = 1.8934, Bias = 0.6405\n",
            "\n",
            "Final Weight: 1.8934, Bias: 0.6405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Vectorization for Speed"
      ],
      "metadata": {
        "id": "KjWz9Df0LNpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* When training our models, we typically want to process whole minibathces of examples simultaneously.\n",
        "* Doing this requires that we vectorixe the calculations and leverage fast linear algebra libraries rather than writing costly-loops in Python."
      ],
      "metadata": {
        "id": "6SQBLwXTL-Ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##example\n",
        "n = 10000 #n-dimensional vector\n",
        "a = torch.ones(n)\n",
        "b = torch.ones(n)\n"
      ],
      "metadata": {
        "id": "AuYXAuXBMUwj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##testing how a for-loop will take\n",
        "c = torch.zeros(n)\n",
        "t = time.time()\n",
        "for i in range(n):\n",
        "  c[i] = a[i] + b[i]\n",
        "f'{time.time()-t:.5f} sec'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Dp289KA0MgoO",
        "outputId": "10cf5539-2aa7-4571-afce-ef916f2a799a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.17138 sec'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##using operator + for elementwise sum\n",
        "t = time.time()\n",
        "d = a+b\n",
        "f\"{time.time() -t:.5f} sec\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1V4fT7aTM0rI",
        "outputId": "1baee80c-f36f-4cd7-e230-50cf6985806e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.00050 sec'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* From above we can conclude that using the operator `+` is faster than the for-loop."
      ],
      "metadata": {
        "id": "neNapfyfNEUJ"
      }
    }
  ]
}