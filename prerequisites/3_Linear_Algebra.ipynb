{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* To start building sophisticated models, we will need a few tools from linear algebra.\n",
        "* Here we will start from basic scalar arithmetic and ramp up to matrix mutliplication."
      ],
      "metadata": {
        "id": "jbne93xZ1tus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Scalars"
      ],
      "metadata": {
        "id": "Qbo2axiN2GVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Most everyday mathematics consits of manipulating numbers one at a time.\n",
        "* We call these values `scalars`.\n",
        "* Scalars are implemented as tensors that contain only one element.\n",
        "* Below, we assign two scalars and perform the familiar addition, multiplication, division and exponentiation operations.\n"
      ],
      "metadata": {
        "id": "SE67gd9v2N0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "X = torch.tensor(3.0)\n",
        "Y = torch.tensor(2.0)\n",
        "X+Y , X*Y, X / Y , X**Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehIKhKlP26cY",
        "outputId": "211e2c7d-ddbd-4467-d3b0-ac2897cf16f3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Vectors"
      ],
      "metadata": {
        "id": "bWzpFx023Yh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Since we're dealing with machine learning you can think of a vector as fixed-length array of scalars.\n",
        "* As their code counterparts, we call these scalrs the elements of the vectors.\n",
        "* When vectors represent examples of real-world datasets, their values hold some real-world significance.\n",
        "* For example, if we were training a model to predict the risk of a loan defaulting, we might associate each applicant with a vector whose components corresponds to quantities like their income, length of employment, or number of previous defaults.\n",
        "\n",
        "*Vectos are implemented as $1^{st}$ order tensors."
      ],
      "metadata": {
        "id": "-dNkDIXs3bvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(3)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfAxgsjg40qw",
        "outputId": "278c46d5-140c-4c57-9e96-7519621e5434"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can refer to an element of a vector by using a subscript. For example, $x_3$ denotes third element of x.\n",
        "* Note that like in Python vector indices start from 0 (zero-based indexing) while linear  algebra subscripts begin at 1 (one-based indexing)."
      ],
      "metadata": {
        "id": "1yG7fkr-45f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASgmgnWM5gKW",
        "outputId": "077b238a-374a-4c44-d881-3fb1e32d896f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* To indicate a vector contains $n$ elements we write $x \\in \\mathbb{R}^n$.\n",
        "* We call $n$ the `dimensionality` of the vector.\n",
        "* In code, this correspondes to the tensor's length, accessible via Python's built-in `len` function."
      ],
      "metadata": {
        "id": "vygcrH4N5uXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LKD7jlH6jm0",
        "outputId": "f6581a9c-82a7-426f-bb18-5e705b7cef53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can also access the length via `shape` attribute.\n",
        "* The shape is atuple that indicatesa tensors's length along each axis.\n",
        "* Tensors with just one axis have shapes with just one element."
      ],
      "metadata": {
        "id": "AOJOiaZn6qMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbGy8bnc6-_c",
        "outputId": "418c4342-8d2a-4fa1-b549-d7b1e64c6429"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* At times `dimension` is gets overloaded with different meanings at times people refer to it as number of axes and the length along a particular axis.\n",
        "* To avoid this confusion, we use `order` to refer to the number of axes and `dimensionality` exclusively to refer to the number of components."
      ],
      "metadata": {
        "id": "38HdGz3u7JEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Matrices"
      ],
      "metadata": {
        "id": "9vX_Hz267qxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Just as scalrs are $O^{th}$ order tensors and vectors are $1^{st}$ tensors, matrices are $2^{nd}$ order tensors.\n",
        "\n",
        "* We denote matrcies by bold letters *X ,Y,Z*, and represent them in code by tensors with two axes.\n",
        "* The expression $A \\in \\mathbb{R}^{m x n}$ indicates that a matrix `A` contains `m x n` real valued scalars, arranged as `m` rows and `n` columnns.\n",
        "* When `m=n`, we say that matriz is `square`.\n",
        "* To refer to a certain element in the matrix we use the subscript $a_{ij}$ showing the value belongs to `A's` $i^{th}$ and $j^{th}$."
      ],
      "metadata": {
        "id": "ILUNR8gt7wQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##constructing a matrix\n",
        "A = torch.arange(6).reshape(3,2)\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIV2eqNmDXyP",
        "outputId": "e8a3f1b6-9fce-44f7-b7b6-e89eb9901e8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [2, 3],\n",
              "        [4, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We may want to flip the axes of the matrix whereby we exchange the matrix's rows and columns, the result is called a `transpose`.\n",
        "* Formally, we signiffy a matrix `A's` transpose by $A^T$."
      ],
      "metadata": {
        "id": "mMSv_fVwDoMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##tranposing matrix A\n",
        "A.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68zmCrGLEDdd",
        "outputId": "ab5994f7-d31d-473e-c7c6-4d22ff3f0e5c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 2, 4],\n",
              "        [1, 3, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Symmetric matrices are subset oF square matrices that are equal to their own tranposes. A = $A^T$"
      ],
      "metadata": {
        "id": "QyvHA19QEJmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.tensor([[1,2,3],[2,0,4],[3,4,5]])\n",
        "A == A.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkQCdoEFEcYP",
        "outputId": "c646794d-af20-4b09-a970-e67d0fea229c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True],\n",
              "        [True, True, True],\n",
              "        [True, True, True]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Tensors"
      ],
      "metadata": {
        "id": "QC4SFRdbEw1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Tensors give us a generic way of describing extensions to $n^{th}$ order arrays.\n",
        "* Tensors become more important when working with images.\n",
        "* Each image arrives as a $3^{rd}$ order tensor with axes corresponding to the height,width and channel.\n",
        "* At each spacial location, the intensities of each color (red,green and blue) are stacked along the channel."
      ],
      "metadata": {
        "id": "A8N9tf9wEz_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(24).reshape(2,3,4)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfMl4lJaFy3l",
        "outputId": "5fec6297-8e03-41b2-d255-0763c84067c9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0,  1,  2,  3],\n",
              "         [ 4,  5,  6,  7],\n",
              "         [ 8,  9, 10, 11]],\n",
              "\n",
              "        [[12, 13, 14, 15],\n",
              "         [16, 17, 18, 19],\n",
              "         [20, 21, 22, 23]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Basic Properties of Tensor Arithmetic"
      ],
      "metadata": {
        "id": "AP_XgBQ7F9Gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Scalars, vectors, matrices and high order tensors allhave some handy properties.\n",
        "* Examples of such operations include: elementwise operations which produce outputs that have the same shape as their operands."
      ],
      "metadata": {
        "id": "5uZPqxRWGFQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.arange(6,dtype=torch.float32).reshape(2,3)\n",
        "B = A.clone() #assign a copy of A and B by allocating a new memory\n",
        "A, A+B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2HnrhWPGd3f",
        "outputId": "98f298bd-3107-4b06-f173-fce8120c02fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 1., 2.],\n",
              "         [3., 4., 5.]]),\n",
              " tensor([[ 0.,  2.,  4.],\n",
              "         [ 6.,  8., 10.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The elementwise product of two matrices is called their `Hadamard product`.\n",
        "* We can spell out the entries of Hadamard product of two matrices `A, B` with:\n",
        "* $A \\odot B =\n",
        "\\begin{bmatrix}\n",
        "a_{11}b_{11} & a_{12}b_{12} & \\dots  & a_{1n}b_{1n} \\\\\n",
        "a_{21}b_{21} & a_{22}b_{22} & \\dots  & a_{2n}b_{2n} \\\\\n",
        "\\vdots       & \\vdots       & \\ddots & \\vdots       \\\\\n",
        "a_{m1}b_{m1} & a_{m2}b_{m2} & \\dots  & a_{mn}b_{mn}\n",
        "\\end{bmatrix}$"
      ],
      "metadata": {
        "id": "TfHT0-kwG36D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A*B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzdpjOneHw6A",
        "outputId": "4f83a039-efe8-4ba2-9f65-ad431f86a10f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  4.],\n",
              "        [ 9., 16., 25.]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Adding or multiplying a scalar and a tensor produces a result with the same shape as the original tensor."
      ],
      "metadata": {
        "id": "5rCRqQO-IflP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = 2\n",
        "X = torch.arange(24).reshape(2,3,4)\n",
        "a + X, (a*X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVx-ylfMG3FZ",
        "outputId": "ef9c45ac-3d80-4f55-e767-c0569d59c11c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 2,  3,  4,  5],\n",
              "          [ 6,  7,  8,  9],\n",
              "          [10, 11, 12, 13]],\n",
              " \n",
              "         [[14, 15, 16, 17],\n",
              "          [18, 19, 20, 21],\n",
              "          [22, 23, 24, 25]]]),\n",
              " torch.Size([2, 3, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Reduction"
      ],
      "metadata": {
        "id": "F2xLVpW4I9Eo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* At times we wish to calculate the sum of a tensor's elements.\n",
        "* To express the sum of the elements in a vector `x` of length `n`, we write $\\sum_{i=1}^{n} x_i$.\n",
        "* There's a simple function in Pytorch which performs this:"
      ],
      "metadata": {
        "id": "_h4C3uLxJY1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(3,dtype=torch.float32)\n",
        "x, x.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SermDnoLJ71Q",
        "outputId": "dbf415a2-2e78-4b37-b78b-079f46e32a0a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 2.]), tensor(3.))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* To express sums over the elements of tensors of arbitrary shape, we simply sum over all its axes.\n",
        "* For instance,the sum of elements of an `m x n` matrix A could be written as $\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$"
      ],
      "metadata": {
        "id": "evNJrq1IKL4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.shape, A.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHe-SbK9KypW",
        "outputId": "5db30593-786d-4353-b1a1-9aa47266762f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 3]), tensor(15.))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Invoking the sum function `reduces` a tensor along all of its axes, eventually producing a scalar.\n",
        "* Pytorch allows us to specify the axes along which the tensor should be reduced.\n",
        "* To sum over all elements along rows (axis 0), we specify axis=0 in the sum function.\n"
      ],
      "metadata": {
        "id": "3PK6axFmK5TU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.shape, A.sum(axis=0).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB4p75lhLc5h",
        "outputId": "1eb3c764-cc39-422d-ce9b-b677f1372822"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 3]), torch.Size([3]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Specifying `axis=1` will reduce the column dimension (axis 1) by summing up elements of all the columns."
      ],
      "metadata": {
        "id": "htVdqXvvLq_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.shape, A.sum(axis=1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Mui6XZZL3Me",
        "outputId": "f44b5ee1-6dc9-4a04-a83c-c995207704fd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 3]), torch.Size([2]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Reducing a matrix along both rows and columns vi summation equivalent to summing up all the elements of the matrix."
      ],
      "metadata": {
        "id": "JQmmqwHFND2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.arange(10).reshape(2,5)\n",
        "##reducing tensor z via summation of columns and rows\n",
        "z.sum(axis=[0,1]) == z.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-i20-czNOyw",
        "outputId": "f0a83936-42fc-4600-aecc-7813e20ec17b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can also used quantities like mean whereby we calculate the mean by dividing the sum by the total number of elements.\n",
        "* Since computing mean is common, it gets a dedicated library function that works analogously to sum."
      ],
      "metadata": {
        "id": "DZkUfP0tNtqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.mean(),A.sum() / A.numel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5U9q6GfODrB",
        "outputId": "f9c03999-f2a4-441e-b596-fff7c4228f5f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(2.5000), tensor(2.5000))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Also the function for calculating the mean can also reduce a tensor along specific axes."
      ],
      "metadata": {
        "id": "xx5FI9tjOPxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3ZF-3Y1OYLi",
        "outputId": "5aa0dd97-2e8e-42b2-c464-6a1464b93beb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1.5000, 2.5000, 3.5000]), tensor([1.5000, 2.5000, 3.5000]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7 Non-Reduction Sum"
      ],
      "metadata": {
        "id": "3z72jxz9POx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Sometimes it can be usedul to keep the number of axes unchanged when invoking the function for calculating the sum or mean.\n",
        "* This matters when we want use the broadcast mechanism."
      ],
      "metadata": {
        "id": "MCbsm1FoQJPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum_A = A.sum(axis=1,keepdims=True)\n",
        "sum_A, sum_A.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev4N9ZOrQg8K",
        "outputId": "498cb648-2cbc-4114-dc58-5d93c66bb453"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 3.],\n",
              "         [12.]]),\n",
              " torch.Size([2, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For instance, since `sum_A` keeps its two axes after summing each row, we can divide `A` y `sum_A` with broadcasting to create a matrix where each row sums up to 1."
      ],
      "metadata": {
        "id": "UccfAq_5R32y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A /sum_A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcA7zw5ySNr7",
        "outputId": "a3ad6d43-0c8c-4110-818e-b6b87501d924"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.3333, 0.6667],\n",
              "        [0.2500, 0.3333, 0.4167]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## calculating cumulative sum of elements of A along\n",
        "## some axis\n",
        "A.cumsum(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a-ChABQScOz",
        "outputId": "db858eec-114e-4c3b-86d6-1788e42f0254"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1., 2.],\n",
              "        [3., 5., 7.]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "j = torch.arange(12).reshape(2,6)\n",
        "j_red_sum = j.sum(axis=0)\n",
        "j_non_sum = j.sum(axis=0,keepdims=True)\n",
        "j, j_red_sum, j_red_sum.shape, j_non_sum, j_non_sum.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1tAP1b7Qv_n",
        "outputId": "0fefc993-d3c3-4c15-e812-e82bf68e11ff"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0,  1,  2,  3,  4,  5],\n",
              "         [ 6,  7,  8,  9, 10, 11]]),\n",
              " tensor([ 6,  8, 10, 12, 14, 16]),\n",
              " torch.Size([6]),\n",
              " tensor([[ 6,  8, 10, 12, 14, 16]]),\n",
              " torch.Size([1, 6]))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.8 Dot Products"
      ],
      "metadata": {
        "id": "-60d0PCYSp7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Dot products is one of the most fundemental operations in linear algebra.\n",
        "* Given two vectos $x, y \\in \\mathbb{R}^d$, their dot product is $x^{T}y$ (also know as inner product,<x,y>) is sum over the products of the elements at the same positions: $x \\cdot y = \\sum_{i=1}^{d} x_i y_i$."
      ],
      "metadata": {
        "id": "PvuahNGQTPiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.ones(3,dtype=torch.float32)\n",
        "x, y, torch.dot(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C7tFCy_XCtq",
        "outputId": "a9b4d7ba-6c0c-412c-d1be-98b824ac00e3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1., 1., 1.]), tensor([1., 1., 1.]), tensor(3.))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can calculate the dot product of two vectos by performing an elementwise multiplication followed by a sum:"
      ],
      "metadata": {
        "id": "VcfJ061mY-G7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(x * y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb_6GW71ZMJH",
        "outputId": "fb45daa3-c95c-40ad-f961-2453c89f36e3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.9 Matrix-Matrix Multiplication\n"
      ],
      "metadata": {
        "id": "N0gUxqxBZe6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Say that we have two matrices $A \\in \\mathbb{R}^{n \\times k}$ and $B \\in \\mathbb{R}^{k \\times m}$:\n",
        "\n",
        "A = \\begin{bmatrix}\n",
        "a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
        "a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "a_{n1} & a_{n2} & \\cdots & a_{nk}\n",
        "\\end{bmatrix}\n",
        "\n",
        "B = \\begin{bmatrix}\n",
        "b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
        "b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "b_{k1} & b_{k2} & \\cdots & b_{km}\n",
        "\\end{bmatrix}"
      ],
      "metadata": {
        "id": "xtHok_ReZvm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We perform matrix-matrix multiplication using torch.matmul:"
      ],
      "metadata": {
        "id": "ZNfbXLsjaL6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## matrix-matrix multiplication\n",
        "b =  torch.rand(3,4)\n",
        "c =torch.rand(4,3)\n",
        "d = torch.matmul(b,c)\n",
        "d_i = b @ c\n",
        "d, d_i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6BsvB5QaRYi",
        "outputId": "0c0f3602-c917-4ca3-d6a9-5183c3cbb424"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.2722, 0.6157, 0.5766],\n",
              "         [0.7616, 0.9073, 0.6931],\n",
              "         [0.9288, 1.7989, 1.5152]]),\n",
              " tensor([[0.2722, 0.6157, 0.5766],\n",
              "         [0.7616, 0.9073, 0.6931],\n",
              "         [0.9288, 1.7989, 1.5152]]))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 Norms"
      ],
      "metadata": {
        "id": "q_Xqwu-hatNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Norm of vector tells how big a vector is.\n",
        "* For instance $ℓ2$ norm measures the (Euclidean) length of a vector.\n",
        "* A norm is a function $||.||$ that maps a vector to a scalar and satisfies the following three properties:\n",
        "   1. Given any vector `x`, if we scale (all elements of) the vector by scalar $\\alpha \\in \\mathbb{R}$ its norm scales accordingly:\n",
        "      \n",
        "      $\\|\\alpha \\mathbf{x}\\| = |\\alpha| \\|\\mathbf{x}\\|$\n",
        "  \n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "KgOI7UtoayS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  2. For any vectos `x` and `y` norms satisfy the triangle inequality.\n",
        "  $\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|$\n",
        "\n",
        "      3. The norm of a vector is nonnegative and it only vanishes if the vector is zero:\n",
        "\n",
        "         $\\|\\mathbf{x}\\| > 0 \\text{ for all } \\mathbf{x} \\neq \\mathbf{0}$"
      ],
      "metadata": {
        "id": "zfo-d7rJdMHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Many functions are valid norms and different norms encode different notions of size.\n",
        "* The Euclidean norm we all learned in high school when calculating the hypotenuse of right angled triangle is the square root of the sum of squares of a vector's elements.\n",
        " * Formally this is called  $ℓ_2$ and is expressed as:\n",
        " $\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^{n} x_i^2}$"
      ],
      "metadata": {
        "id": "Xd8cDhmXei9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##calculating l2 norm\n",
        "u = torch.tensor([3.0,-4.0])\n",
        "torch.norm(u)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYFnwzp0flQE",
        "outputId": "8c2ac41c-9502-4f82-9e01-ad858ccb96e6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The  $ ℓ_1$  norm measures the Manhattan distance.\n",
        "* The  $ ℓ_1 $ norm sums the absolute values of a vector's elements.\n",
        "$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^{n} |x_i|$.\n",
        "* The L1 norm is less sensitive to outliers. To compute the L1 norm, we compose the absolute value with the sum operation.\n"
      ],
      "metadata": {
        "id": "Slc3Jy0Of1B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.abs(u).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL5AUP7khD3y",
        "outputId": "b778afdf-184f-4d65-9bba-ae3b120ac23c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Both the L2 and L1 norms are special cases of the general of Lp norms:\n",
        "$\\|\\mathbf{x}\\|_p = \\left( \\sum_{i=1}^{n} |x_i|^p \\right)^{1/p}$.\n",
        "* In the case of matrices, matters are more complicated. After all, matrices can be viewed\n",
        "both ascollections of individual entries and as objects that operate on vectors and transform\n",
        "them into other vectors. For instance, we can ask by how much longer the matrix–vector\n",
        "product Xv could be relative to v. This line of thought leads to what is called the spectral norm.\n",
        "* For now we introduce the `Frobenius norm`, which is much easier to compute and defined as the square root sum of the squares of a matrix's elements.\n",
        "\n",
        "$\\|A\\|_F = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}^2}$\n",
        "\n",
        "* We apply these norms when solving optimization problems: like say we want to maximize the probability assigned to observed data, maximize the revenue associated with a recommender model, minimize the distance between predictions and the ground truth observations etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "AT0Uq5vNhIS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.norm(torch.ones((4,9)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbvDl-ssjWTy",
        "outputId": "7312b311-4274-4867-d3c8-cd2d4a55c076"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}