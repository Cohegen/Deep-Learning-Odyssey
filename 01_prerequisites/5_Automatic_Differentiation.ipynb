{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* All modern deep learning frameworks offer `automatic differentiation (autograd)`, as we pass data through each succesive function, the framework builds a `computational graph` that tracks how each value depends on others.\n",
        "* To calculate derivatives, automatic differentiation works backwards through this graph applying the chain rule.\n",
        "* The computational algorithm for applying the chain rule is called *backpropagation*."
      ],
      "metadata": {
        "id": "UKTHTvJbZFC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "CRrzBzfvZLXu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 A Simple Function"
      ],
      "metadata": {
        "id": "Q_LozRX8ZQ0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's assume that we are differentiating the function $y=2x^Tx$"
      ],
      "metadata": {
        "id": "X28EVuHyZVJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(4.0)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8ebpKC2Zikv",
        "outputId": "057ab627-9dce-46cf-b1df-dff01dc10b4f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Before calculating the gradient of $y$ with respect to $x$, we need a place to store it.\n",
        "* In general, we want to avoid allocating new memory everytime we take a derivative because deep learning requires successively computing derivatives with respect to the same parameters a great many times, and we might risk running out of memory."
      ],
      "metadata": {
        "id": "Ybiuxg43Zp26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.requires_grad_(True)\n",
        "x.grad#the gradient is None by default"
      ],
      "metadata": {
        "id": "_sBykQbIaMT6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##calculating our function of x and assigning\n",
        "##results to y\n",
        "y = 2*torch.dot(x,x)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjM4jtrmacTj",
        "outputId": "940da574-0274-4943-e4d3-4e88d1bad857"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(28., grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##taking gradient of y with respect to x by calling\n",
        "## its backward method\n",
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyKroQklarEK",
        "outputId": "7fbcc367-c2cf-446a-8eab-40557d8c21dd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##verifying the automatic gradient calculation and the expected result are\n",
        "## identical\n",
        "x.grad == 4*x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFdLczWmbANv",
        "outputId": "4e3fa537-4ad4-4ecf-db00-3b9e19988829"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK2t-ETZcIRw",
        "outputId": "8cf27fa5-133a-4c1e-bc98-b9775e4ee14b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Note that Pytorch does not automatically reset the gradient buffer when we record new gradient.\n",
        "* Instead, the new gradient is added to the already-stored gradient.\n",
        "* This behavior comes in handy when we want to optimize the sum of multiple objective functions.\n",
        "* To reset the gradient buffer, we can call `x.grad.zero_()` as follows:"
      ],
      "metadata": {
        "id": "FWQkXCBvbfCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_() #reset gradients\n",
        "y = x.sum()\n",
        "y.backward()\n",
        "x.grad\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaZFLST3cAjl",
        "outputId": "abd2a8b3-51d0-49ec-fb56-902383fe7388"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##extended example\n",
        "#creating our tensors and enabling gradient tracking\n",
        "x = torch.tensor(2.0,requires_grad=True)\n",
        "y = torch.tensor(3.0,requires_grad=True)\n",
        "\n",
        "## definiing the function z (forward pass)\n",
        "z = x**3 + 4*y\n",
        "\n",
        "#calculating the gradients(backward pass)\n",
        "z.backward()\n",
        "\n",
        "#access the results\n",
        "print(f\"Gradient of x: {x.grad}\")\n",
        "print(f\"Gradient of y: {y.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnKPjhuGdgzE",
        "outputId": "8ad3d2b2-b4ca-47b6-b5f9-c62d75ef203a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of x: 12.0\n",
            "Gradient of y: 4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* You may be curious as to why we need to declare `z.backward()` instead of just declaring only `x.grad , y.grad`.\n",
        "* This is because when you ddefine `z=x**3 + 4*y`, Pytorch does not solve the calculus problem symbolically.\n",
        "* Instead, it only knows the `local` relationships. It knows that `z` was created by addition of two things.\n",
        "* Before `z.backward()`: `x.grad` is `None`. The memory for the gradient hasn't even been allocated yet.\n",
        "* After `z.backward()`: Pytorch starts at `z` looks at its history, see $x$ and $y$, calculates the numerical values of the derivatives, and \"deposits\" those values into the `.grad` buckets of $x$ and $y$."
      ],
      "metadata": {
        "id": "hU9SPh6MfL_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Backward for Non-Scalar Variables.\n"
      ],
      "metadata": {
        "id": "Y5-2aqP5hji5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* When $y$ is a vector, the most natural representation of the derivative of $y$ with respect to a vector $x$ is a matrix called the *Jacobian* that contains the partial derivatives of each component of *y* with respect to each component of *x*.\n"
      ],
      "metadata": {
        "id": "jcLFBIyPhtEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#x = torch.arange(4.0)\n",
        "x.grad.zero_()\n",
        "y = x*x\n",
        "y.backward(gradient=torch.ones(len(y)))\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmjI7pomigWt",
        "outputId": "2c51f0d3-9ad1-4ddd-9b05-919ac6489047"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Detaching Computation"
      ],
      "metadata": {
        "id": "gjN5zGWDjU6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Suppose we wish to move some calculations outside of the recorded computational graph.\n",
        "* For example, say that we use the input to create some auxiliary intermediate terms for which we do not want to compute a gradient.\n",
        "* Therefore we need to `detach` the respective computational graph from the final result.\n",
        "* Suppose we have `z = x*y` and `y=x*x` but we want to focus on the direct influence of `x` and `z` rather than the influence conveyed via y.\n",
        "* In this case we can create a variable `u` that takes the same value as `y` but whose `provenance(how it has been created)` has been wiped out.\n",
        "* So `u` has no ancestors in the graph and gradients do not flow through `u` to `x`.\n",
        "* Taking the gradient of `z= x*u` will yield the result `u`,(not 3*x*x) as you might have expected since `z=x*x*x`."
      ],
      "metadata": {
        "id": "KZYOs8eOjy77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "y = x*x\n",
        "u = y.detach()\n",
        "z = u*x\n",
        "\n",
        "z.sum().backward()\n",
        "x.grad == u"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCNMyofTl0b8",
        "outputId": "926b2e45-21c2-429d-a80a-f21bb56d668f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##calculating gradient of y with respect to x\n",
        "x.grad.zero_()\n",
        "y.sum().backward()\n",
        "x.grad == 2*x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTOH4COsmRtW",
        "outputId": "bbc58103-00fd-49d5-abb0-c0826a8794fe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Gradients and Python Control Flow\n"
      ],
      "metadata": {
        "id": "d9Sig9bzmjEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Programming offers us a lot more freedom in how we compute results of deriving functions.\n",
        "* We can make them depend on auxiliary variables or condition choices on intermediate results.\n",
        "* One benefit of using automatic differentiation is that even if building computational graph of a function required passing through  maze of Python control flow, we can still calculate the gradient of the resulting variable."
      ],
      "metadata": {
        "id": "1oyFlFAgmn2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(a):\n",
        "  b = a*2\n",
        "  while b.norm() <1000:\n",
        "    b = b**2\n",
        "  if b.sum() > 0:\n",
        "    c = b\n",
        "  else:\n",
        "    c = 100* b\n",
        "  return c\n",
        ""
      ],
      "metadata": {
        "id": "v5oVnRN7niHe"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##calling the function by passing in a random value, as input\n",
        "a = torch.randn(size=(),requires_grad=True)\n",
        "d = f(a)\n",
        "d.backward()"
      ],
      "metadata": {
        "id": "3_xz1Z3-n32m"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Since the input is a random variable, we do not know what form the computational graph will take.\n"
      ],
      "metadata": {
        "id": "tqsItlj3m3Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.grad == d / a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRkiSqD2ov_u",
        "outputId": "c48b9232-b1ca-47bb-9571-b2ca30eb60b7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(False)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(False)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}