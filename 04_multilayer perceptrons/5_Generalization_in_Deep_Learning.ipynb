{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Revisting Overffitting and Regularization."
      ],
      "metadata": {
        "id": "5aqJALWnXmTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Given a finite set of training examples, a model relies on certain assumptions, to achieve human-level performace it may be useful to identify **inductive biases** that reflect how humans think about the world.\n",
        "* With machine learning models encoding inductive biases, our approach to training them typically consists of two phases:\n",
        "  1. fit the training data\n",
        "  2. Estimate the generaliation error ny evaluating the model on the validation data.\n",
        "* The difference between the model's performance on training and test data is called **generalization gap**.\n",
        "* When this gap is too large we say that our model is **overfitting** to the training data.\n",
        "*For many deep learning tasks e.g image recognition we typically choose among model architectures, all of which can achieve relatively low training loss.\n",
        "* We can reduce the genealization error in deep learing models by making them more **expressive** e.g by adding more layers, nodes or increasing the number of epochs."
      ],
      "metadata": {
        "id": "glSYNyQbXwHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Inspiration from Nonparametrics."
      ],
      "metadata": {
        "id": "zDoOYUfHamKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* While neural networks have parameters,in some ways it can be more fruitful to think of them as behaving like nonparametric models.\n",
        "* Nonparametric methods tend to have a level of complexity that grows as the amount of avaible data grows.\n",
        "* The simplest example of nonparametric model is k-nearest neighbor algorithm.\n",
        "* During training time, the learner simply memorizes the dataset.\n",
        "* Then at prediction time, when confroneted with a new point **x** the model looks up the $k$ nearest neighbors.\n",
        "* When $k=1$ the algorithm is called 1-nearest neighbor, and the algoritm will no generalize.\n",
        "* Infact it turns out under mild conditions, the 1-nearest neighbor algorithm is consistent since it eventually converges to the optimal predictor.\n",
        "* 1-nearesr neighbor requies choosing a distance metric or a feature representation of the data.\n",
        "* No matter which distance metric used 1-nearest neighbor can fit the training data and will approach the optimal predictor as data grows.\n",
        "* However, with limited data, different distance metric leads to different predictions because each metric encoded a different inductive bias(assumption about what patterns matter).\n",
        "* The model's performance depends on how well these assumptions match the true structure of the data."
      ],
      "metadata": {
        "id": "kWoZ0z6warsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2 Early Stopping"
      ],
      "metadata": {
        "id": "tm1hnpYiedRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* While deep neural networks are capable of fitting random labels even when labels are assigned incorrectly or randomly this capability emerges over any iterations of training.\n",
        "* During training, they first learn clean,meaningful patterns in the data and only later start fitting noisy or incorrect labels.\n",
        "* Importantly,when a model fits the clean data but not the random labels, it is already generalizing well.\n",
        "* This behaviour motivates ***early stopping** as a regularization technique.\n",
        "* Insted of restricting model weights, early stopping  limits how long training runs.\n",
        "* Typically, training is stopped when the validation error stops improving beyond a small threshold for several epochs.\n",
        "* The two benefits of early stopping is:\n",
        "  1. Bettern generalization when labels are noisy or inherently uncertain.\n",
        "  2. Significant savings in training time and computation cost.\n",
        "\n",
        "* Note that when data is clean and perfectly seperable,early stopping offer little benefit. But in the presence of label or instrinsic uncertainity, continuing trainig until the model fits the noise usually harms performance, making early stopping crucial."
      ],
      "metadata": {
        "id": "gpFDMPp1egls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Classical Regularization Methods for Deep networks."
      ],
      "metadata": {
        "id": "mIeNxxQRhyAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Classical regularization methods, such as **weight decay** reduce model complexity by adding a penalty on large weights e.g $l_2$ (ridge regularization) or $l_1$ (lasso regularization).\n",
        "* In deep learning, weight decay is still widely used but in practice typical $l_2$ regularization is too weak to stop neural networks from perfectly fitting the training data.\n",
        "* As a result, its benefits oftan only when combined with early stopping.\n",
        "* Withot early stopping, weight decay may not truly limit model capacity.\n",
        "* Instead,these techniques may help generalization by introducing inductive biases similar to choices like network depth number of neurons or distance metrics in k-NN rather than by directly restricting model expressiveness.\n",
        "* Despite the unclear explanation, classical regularizers remain popular.\n",
        "* Deep learning has adapted techniques like **dropout** which is widely sued even though its theorectical foundations are still not fully understood."
      ],
      "metadata": {
        "id": "5F2D9Hs_h4WT"
      }
    }
  ]
}