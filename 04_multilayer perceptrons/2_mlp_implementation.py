# -*- coding: utf-8 -*-
"""2_MLP_Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16gXQKv3eC0fGrbpBsfri8P9B8GiGWK-q
"""

!pip install d2l --no-deps

import torch
from torch import nn
from d2l import torch as d2l

"""## 1.1 Implementation from scratch

# Initializing Model Parameters.

* Recall the Fahion-MNIST dataset has 10 classes,and each image consists of  a 28x28 = 784 grid of grayscale pixel values.
* We also disregard the spatila structure among pixels for now, so we can think of this as a classification dataset with 784 features and 10 classes.
* We will implement an MLP with one hidden layer and 256 hidden units.
* We choose the layer widths to be divisible by larger powers of 2.
* This is computationally efficient due to the way memory is allocated and addressed in hardware.
* We will represent our parameters with several tensors.
* Note that for every layer, we must keep track of one weight matrix and one bias vector.
* We also alllocate memory for gradients of loss with respect to these parameters.
"""

class SimpleMLP(d2l.Classifier):
  def __init__(self,num_inputs,num_outputs,num_hiddens,lr,sigma=0.01):
    super().__init__()
    self.save_hyperparameters()
    self.w1 = nn.Parameter(torch.randn(num_inputs,num_hiddens)*sigma)
    self.b1 =nn.Parameter(torch.zeros(num_hiddens))
    self.w2 = nn.Parameter(torch.randn(num_hiddens,num_outputs)*sigma)
    self.b2 = nn.Parameter(torch.zeros(num_outputs))

  def forward(self,X):
    X = X.reshape((-1,self.num_inputs))
    H = relu(torch.matmul(X,self.w1)+self.b1)
    return torch.matmul(H,self.w2)+self.b2

"""## Model

* To make sure we know everything works, we will implement the ReLU activation function rather than invoking the built-in relu function.
"""

def relu(X):
  a = torch.zeros_like(X)
  return torch.max(X,a)

"""* Since we are disregarding spatial structure, we reshape each two dimensional image into a flat vector of length **num_inputs**."""



"""## Training

"""

model = SimpleMLP(num_inputs=784,num_outputs=10,num_hiddens=256,lr=0.1)
data = d2l.FashionMNIST(batch_size=256)
trainer = d2l.Trainer(max_epochs=10)
trainer.fit(model,data)

"""## Concise Implementation

## Model
"""

class MLP(d2l.Classifier):
  def __init__(self,num_outputs,num_hiddens,lr):
    super().__init__()
    self.save_hyperparameters()
    self.net = nn.Sequential(
        nn.Flatten(),
        nn.LazyLinear(num_hiddens),
        nn.ReLU(),
        nn.LazyLinear(num_outputs)
    )

"""* The `Sequential` class abstracts the forward process enabling us to focus on the transformations.

## Training
"""

model = MLP(num_outputs=10,num_hiddens=512,lr=0.1)
trainer.fit(model,data)