{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* In this section, we take a deep dive into details of **backward propagation** also known as **backpropagation**."
      ],
      "metadata": {
        "id": "uHkQ-ZLdNd2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Forward Propagation."
      ],
      "metadata": {
        "id": "HRKxCps7N1Bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **forward propagation** is the calculation and storage of intermediate variables for a neural network in order from the input layer to the output layer.\n",
        "\n",
        "* Let's assume that the input example is **x** and that our hidden layers does not include a bias term.\n",
        "* Here the intermediate variable is:\n",
        "  $z = W^{(1)}x$.\n",
        "* Here  $ W^{(1)}$ is the weight parameter of the hidden layer."
      ],
      "metadata": {
        "id": "u5QunzGtN5cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "kRZBzuOZPg8s"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.randn(4,4)"
      ],
      "metadata": {
        "id": "0RI9_yM9TioK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(3,4,requires_grad=True)\n",
        "w_1 = torch.rand(4,3,requires_grad=True)\n",
        "z = torch.matmul(w_1,x)\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soBojpaePo8x",
        "outputId": "1fbe3653-9f82-4e5b-cb90-c70cace08ad4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3206, 0.7111, 0.4913, 0.4429],\n",
              "        [0.8187, 1.4006, 0.9113, 1.1031],\n",
              "        [0.4330, 0.3157, 0.2539, 0.3526],\n",
              "        [0.9858, 1.2758, 0.9573, 0.9735]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* After running intermiadiate variable $z$ through activation function $Œ¶$ we obtain our hidden activation vector of length $h$:\n",
        "\n",
        "$h = Œ¶ (z)$."
      ],
      "metadata": {
        "id": "YUqoQ0y6Qca8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(X):\n",
        "  return torch.max(torch.tensor(0.0),X)"
      ],
      "metadata": {
        "id": "I-d2odxwRMDU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = relu(z)\n",
        "h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI-5LYQARXn9",
        "outputId": "b7911d63-4da1-408b-bb64-584e6ec45ac3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3206, 0.7111, 0.4913, 0.4429],\n",
              "        [0.8187, 1.4006, 0.9113, 1.1031],\n",
              "        [0.4330, 0.3157, 0.2539, 0.3526],\n",
              "        [0.9858, 1.2758, 0.9573, 0.9735]], grad_fn=<MaximumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The hidden layer output $h$ is also an intermediate variable.\n",
        "* Assumng that the parameters of the output layer posses only weight of $ W^{(2)}$, we can obtain an output layer variable with a vector of length q:\n",
        "   \n",
        "   $\n",
        "   o =  W^{(2)} h\n",
        "   $"
      ],
      "metadata": {
        "id": "coxIkfsHSgwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_2 = torch.rand(4,4,requires_grad=True)\n",
        "o = torch.matmul(w_2,h)\n",
        "o"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wpxxZqPTERd",
        "outputId": "356c86c1-6240-4e4a-b584-75b63652e242"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.9489, 2.8558, 1.9930, 2.2344],\n",
              "        [0.6234, 0.8544, 0.6264, 0.6488],\n",
              "        [1.7671, 2.6867, 1.8752, 2.0527],\n",
              "        [1.1917, 2.0141, 1.3648, 1.5079]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Assuming the loss function $l$ and the example label is $y$, we can can calculate the loss term for a single data example:\n",
        "  $L = l(o,y)$"
      ],
      "metadata": {
        "id": "tteTuL1UTxEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y_true,y_hat):\n",
        "  ##ensuring values of y_hat are positive for log\n",
        "  #y_hat_clipped = torch.clamp(y_hat,min=1e-8) #avoids log(0)\n",
        "  return 0.5*torch.mean((y_hat-y_true)**2)"
      ],
      "metadata": {
        "id": "214G6UXeUFNB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L=loss(y,o)\n",
        "L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pwjBmxHUMxm",
        "outputId": "ca52de67-c200-47d3-ae3b-2e1003bea6be"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.5880, grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Given the hyperparameter $Œª$, the regularization term is:\n",
        "  \n",
        "  $s = \\frac{\\lambda}{2} \\left( \\|W^{(1)}\\|_F^2 + \\|W^{(2)}\\|_F^2 \\right)$\n",
        "\n",
        "* where the Frobenius norm of a matrix is simply the $l_2$.\n",
        "* Finally, the model's regularized loss on a given data example is:\n",
        "  $J = L + s$\n",
        "* We refer to $J$ as the $objective function$."
      ],
      "metadata": {
        "id": "ex5mLcUjUirv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##regularization term\n",
        "lambda_ = 0.1\n",
        "s = lambda_*0.5*(torch.norm(w_1,p='fro') + torch.norm(w_2,p='fro'))\n",
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9iHKLPRV2GC",
        "outputId": "01015a10-adca-4f81-bbc4-be1e731c673d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.2149, grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##objective function\n",
        "J =L + s\n",
        "J"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHprKdfJWiGF",
        "outputId": "f80ae946-e264-4831-a34b-55f77072645f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.8030, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Computational Graph of Forward Propagation\n"
      ],
      "metadata": {
        "id": "rOmz8RARW5rg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7JPBcivzXUUo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7363ce73"
      },
      "source": [
        "# Task\n",
        "Install `torchviz`, re-initialize the tensors `x`, `w_1`, and `w_2` with `requires_grad=True`, and then re-run the forward propagation to ensure the computation graph is traceable. Finally, use `torchviz.make_dot` to plot the computational graph of the objective function `J`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01b54807"
      },
      "source": [
        "## Install torchviz\n",
        "\n",
        "### Subtask:\n",
        "Install the torchviz library for visualizing PyTorch computation graphs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "423fa62e"
      },
      "source": [
        "**Reasoning**:\n",
        "To install the `torchviz` library as per the instructions, I will use `!pip install torchviz` in a code cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adbaa968",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e13a10b9-dae7-4b7e-90a9-7fff7645b2ac"
      },
      "source": [
        "get_ipython().system('pip install torchviz')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torchviz) (2.9.0+cu126)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from torchviz) (0.21)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torchviz) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torchviz) (3.0.3)\n",
            "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "# Plot the computational graph of J\n",
        "graph = make_dot(J, params={'x': x, 'w_1': w_1, 'w_2': w_2,})\n",
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "id": "7Id7-nDLZQic",
        "outputId": "ae1b2e90-8af2-4fe1-b25f-d551b5c4e5af"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"402pt\" height=\"611pt\"\n viewBox=\"0.00 0.00 402.00 611.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 607)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-607 398,-607 398,4 -4,4\"/>\n<!-- 132699686264160 -->\n<g id=\"node1\" class=\"node\">\n<title>132699686264160</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"163.5,-31 109.5,-31 109.5,0 163.5,0 163.5,-31\"/>\n<text text-anchor=\"middle\" x=\"136.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 132699697544976 -->\n<g id=\"node2\" class=\"node\">\n<title>132699697544976</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"181,-86 92,-86 92,-67 181,-67 181,-86\"/>\n<text text-anchor=\"middle\" x=\"136.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n</g>\n<!-- 132699697544976&#45;&gt;132699686264160 -->\n<g id=\"edge20\" class=\"edge\">\n<title>132699697544976&#45;&gt;132699686264160</title>\n<path fill=\"none\" stroke=\"black\" d=\"M136.5,-66.79C136.5,-60.07 136.5,-50.4 136.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"140,-41.19 136.5,-31.19 133,-41.19 140,-41.19\"/>\n</g>\n<!-- 132699704599136 -->\n<g id=\"node3\" class=\"node\">\n<title>132699704599136</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"145,-141 56,-141 56,-122 145,-122 145,-141\"/>\n<text text-anchor=\"middle\" x=\"100.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 132699704599136&#45;&gt;132699697544976 -->\n<g id=\"edge1\" class=\"edge\">\n<title>132699704599136&#45;&gt;132699697544976</title>\n<path fill=\"none\" stroke=\"black\" d=\"M106.44,-121.75C111.48,-114.34 118.84,-103.5 125.01,-94.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"127.94,-96.33 130.67,-86.09 122.15,-92.39 127.94,-96.33\"/>\n</g>\n<!-- 132699680135472 -->\n<g id=\"node4\" class=\"node\">\n<title>132699680135472</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"130,-196 35,-196 35,-177 130,-177 130,-196\"/>\n<text text-anchor=\"middle\" x=\"82.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">MeanBackward0</text>\n</g>\n<!-- 132699680135472&#45;&gt;132699704599136 -->\n<g id=\"edge2\" class=\"edge\">\n<title>132699680135472&#45;&gt;132699704599136</title>\n<path fill=\"none\" stroke=\"black\" d=\"M85.47,-176.75C87.86,-169.72 91.29,-159.62 94.27,-150.84\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"97.68,-151.68 97.58,-141.09 91.05,-149.43 97.68,-151.68\"/>\n</g>\n<!-- 132699680135808 -->\n<g id=\"node5\" class=\"node\">\n<title>132699680135808</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"119,-251 30,-251 30,-232 119,-232 119,-251\"/>\n<text text-anchor=\"middle\" x=\"74.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">PowBackward0</text>\n</g>\n<!-- 132699680135808&#45;&gt;132699680135472 -->\n<g id=\"edge3\" class=\"edge\">\n<title>132699680135808&#45;&gt;132699680135472</title>\n<path fill=\"none\" stroke=\"black\" d=\"M75.82,-231.75C76.87,-224.8 78.37,-214.85 79.69,-206.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"83.17,-206.5 81.2,-196.09 76.25,-205.45 83.17,-206.5\"/>\n</g>\n<!-- 132699680135856 -->\n<g id=\"node6\" class=\"node\">\n<title>132699680135856</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"103,-306 14,-306 14,-287 103,-287 103,-306\"/>\n<text text-anchor=\"middle\" x=\"58.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\">SubBackward0</text>\n</g>\n<!-- 132699680135856&#45;&gt;132699680135808 -->\n<g id=\"edge4\" class=\"edge\">\n<title>132699680135856&#45;&gt;132699680135808</title>\n<path fill=\"none\" stroke=\"black\" d=\"M61.14,-286.75C63.26,-279.72 66.31,-269.62 68.96,-260.84\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"72.37,-261.67 71.91,-251.09 65.67,-259.65 72.37,-261.67\"/>\n</g>\n<!-- 132699680135952 -->\n<g id=\"node7\" class=\"node\">\n<title>132699680135952</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"98,-361 15,-361 15,-342 98,-342 98,-361\"/>\n<text text-anchor=\"middle\" x=\"56.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\">MmBackward0</text>\n</g>\n<!-- 132699680135952&#45;&gt;132699680135856 -->\n<g id=\"edge5\" class=\"edge\">\n<title>132699680135952&#45;&gt;132699680135856</title>\n<path fill=\"none\" stroke=\"black\" d=\"M56.83,-341.75C57.09,-334.8 57.47,-324.85 57.8,-316.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"61.3,-316.21 58.18,-306.09 54.3,-315.95 61.3,-316.21\"/>\n</g>\n<!-- 132699680136048 -->\n<g id=\"node8\" class=\"node\">\n<title>132699680136048</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"232,-416 131,-416 131,-397 232,-397 232,-416\"/>\n<text text-anchor=\"middle\" x=\"181.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 132699680136048&#45;&gt;132699680135952 -->\n<g id=\"edge6\" class=\"edge\">\n<title>132699680136048&#45;&gt;132699680135952</title>\n<path fill=\"none\" stroke=\"black\" d=\"M161.42,-396.98C141.13,-388.38 109.54,-374.99 86.33,-365.15\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"87.52,-361.85 76.95,-361.17 84.79,-368.3 87.52,-361.85\"/>\n</g>\n<!-- 132699680135568 -->\n<g id=\"node19\" class=\"node\">\n<title>132699680135568</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"283,-361 116,-361 116,-342 283,-342 283,-361\"/>\n<text text-anchor=\"middle\" x=\"199.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\">LinalgVectorNormBackward0</text>\n</g>\n<!-- 132699680136048&#45;&gt;132699680135568 -->\n<g id=\"edge19\" class=\"edge\">\n<title>132699680136048&#45;&gt;132699680135568</title>\n<path fill=\"none\" stroke=\"black\" d=\"M184.47,-396.75C186.86,-389.72 190.29,-379.62 193.27,-370.84\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"196.68,-371.68 196.58,-361.09 190.05,-369.43 196.68,-371.68\"/>\n</g>\n<!-- 132703847003424 -->\n<g id=\"node9\" class=\"node\">\n<title>132703847003424</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"209,-482 150,-482 150,-452 209,-452 209,-482\"/>\n<text text-anchor=\"middle\" x=\"179.5\" y=\"-470\" font-family=\"monospace\" font-size=\"10.00\">w_2</text>\n<text text-anchor=\"middle\" x=\"179.5\" y=\"-459\" font-family=\"monospace\" font-size=\"10.00\"> (4, 4)</text>\n</g>\n<!-- 132703847003424&#45;&gt;132699680136048 -->\n<g id=\"edge7\" class=\"edge\">\n<title>132703847003424&#45;&gt;132699680136048</title>\n<path fill=\"none\" stroke=\"black\" d=\"M179.98,-451.84C180.24,-444.21 180.57,-434.7 180.85,-426.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"184.36,-426.38 181.2,-416.27 177.36,-426.14 184.36,-426.38\"/>\n</g>\n<!-- 132699680136096 -->\n<g id=\"node10\" class=\"node\">\n<title>132699680136096</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"113,-416 0,-416 0,-397 113,-397 113,-416\"/>\n<text text-anchor=\"middle\" x=\"56.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\">MaximumBackward0</text>\n</g>\n<!-- 132699680136096&#45;&gt;132699680135952 -->\n<g id=\"edge8\" class=\"edge\">\n<title>132699680136096&#45;&gt;132699680135952</title>\n<path fill=\"none\" stroke=\"black\" d=\"M56.5,-396.75C56.5,-389.8 56.5,-379.85 56.5,-371.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"60,-371.09 56.5,-361.09 53,-371.09 60,-371.09\"/>\n</g>\n<!-- 132699680136240 -->\n<g id=\"node11\" class=\"node\">\n<title>132699680136240</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"115,-476.5 32,-476.5 32,-457.5 115,-457.5 115,-476.5\"/>\n<text text-anchor=\"middle\" x=\"73.5\" y=\"-464.5\" font-family=\"monospace\" font-size=\"10.00\">MmBackward0</text>\n</g>\n<!-- 132699680136240&#45;&gt;132699680136096 -->\n<g id=\"edge9\" class=\"edge\">\n<title>132699680136240&#45;&gt;132699680136096</title>\n<path fill=\"none\" stroke=\"black\" d=\"M70.99,-457.37C68.61,-449.16 64.94,-436.54 61.89,-426.05\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"65.17,-424.79 59.02,-416.17 58.45,-426.75 65.17,-424.79\"/>\n</g>\n<!-- 132699680136288 -->\n<g id=\"node12\" class=\"node\">\n<title>132699680136288</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"302,-537 201,-537 201,-518 302,-518 302,-537\"/>\n<text text-anchor=\"middle\" x=\"251.5\" y=\"-525\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 132699680136288&#45;&gt;132699680136240 -->\n<g id=\"edge10\" class=\"edge\">\n<title>132699680136288&#45;&gt;132699680136240</title>\n<path fill=\"none\" stroke=\"black\" d=\"M225.62,-517.99C194.9,-507.9 143.52,-491.01 109.08,-479.69\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"110.01,-476.31 99.41,-476.52 107.82,-482.96 110.01,-476.31\"/>\n</g>\n<!-- 132699680136000 -->\n<g id=\"node18\" class=\"node\">\n<title>132699680136000</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"394,-476.5 227,-476.5 227,-457.5 394,-457.5 394,-476.5\"/>\n<text text-anchor=\"middle\" x=\"310.5\" y=\"-464.5\" font-family=\"monospace\" font-size=\"10.00\">LinalgVectorNormBackward0</text>\n</g>\n<!-- 132699680136288&#45;&gt;132699680136000 -->\n<g id=\"edge17\" class=\"edge\">\n<title>132699680136288&#45;&gt;132699680136000</title>\n<path fill=\"none\" stroke=\"black\" d=\"M260.21,-517.87C269.18,-508.97 283.38,-494.89 294.41,-483.95\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"297.12,-486.19 301.76,-476.67 292.19,-481.22 297.12,-486.19\"/>\n</g>\n<!-- 132704090443760 -->\n<g id=\"node13\" class=\"node\">\n<title>132704090443760</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"281,-603 222,-603 222,-573 281,-573 281,-603\"/>\n<text text-anchor=\"middle\" x=\"251.5\" y=\"-591\" font-family=\"monospace\" font-size=\"10.00\">w_1</text>\n<text text-anchor=\"middle\" x=\"251.5\" y=\"-580\" font-family=\"monospace\" font-size=\"10.00\"> (4, 3)</text>\n</g>\n<!-- 132704090443760&#45;&gt;132699680136288 -->\n<g id=\"edge11\" class=\"edge\">\n<title>132704090443760&#45;&gt;132699680136288</title>\n<path fill=\"none\" stroke=\"black\" d=\"M251.5,-572.84C251.5,-565.21 251.5,-555.7 251.5,-547.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"255,-547.27 251.5,-537.27 248,-547.27 255,-547.27\"/>\n</g>\n<!-- 132699680136336 -->\n<g id=\"node14\" class=\"node\">\n<title>132699680136336</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"124,-537 23,-537 23,-518 124,-518 124,-537\"/>\n<text text-anchor=\"middle\" x=\"73.5\" y=\"-525\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 132699680136336&#45;&gt;132699680136240 -->\n<g id=\"edge12\" class=\"edge\">\n<title>132699680136336&#45;&gt;132699680136240</title>\n<path fill=\"none\" stroke=\"black\" d=\"M73.5,-517.87C73.5,-509.75 73.5,-497.31 73.5,-486.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"77,-486.67 73.5,-476.67 70,-486.67 77,-486.67\"/>\n</g>\n<!-- 132704090441360 -->\n<g id=\"node15\" class=\"node\">\n<title>132704090441360</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"103,-603 44,-603 44,-573 103,-573 103,-603\"/>\n<text text-anchor=\"middle\" x=\"73.5\" y=\"-591\" font-family=\"monospace\" font-size=\"10.00\">x</text>\n<text text-anchor=\"middle\" x=\"73.5\" y=\"-580\" font-family=\"monospace\" font-size=\"10.00\"> (3, 4)</text>\n</g>\n<!-- 132704090441360&#45;&gt;132699680136336 -->\n<g id=\"edge13\" class=\"edge\">\n<title>132704090441360&#45;&gt;132699680136336</title>\n<path fill=\"none\" stroke=\"black\" d=\"M73.5,-572.84C73.5,-565.21 73.5,-555.7 73.5,-547.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"77,-547.27 73.5,-537.27 70,-547.27 77,-547.27\"/>\n</g>\n<!-- 132699680135760 -->\n<g id=\"node16\" class=\"node\">\n<title>132699680135760</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"241,-196 152,-196 152,-177 241,-177 241,-196\"/>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 132699680135760&#45;&gt;132699697544976 -->\n<g id=\"edge14\" class=\"edge\">\n<title>132699680135760&#45;&gt;132699697544976</title>\n<path fill=\"none\" stroke=\"black\" d=\"M191.59,-176.66C181.7,-158.85 159.2,-118.37 146.23,-95.01\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"149.23,-93.2 141.31,-86.16 143.11,-96.6 149.23,-93.2\"/>\n</g>\n<!-- 132699680135904 -->\n<g id=\"node17\" class=\"node\">\n<title>132699680135904</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"244,-306 155,-306 155,-287 244,-287 244,-306\"/>\n<text text-anchor=\"middle\" x=\"199.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n</g>\n<!-- 132699680135904&#45;&gt;132699680135760 -->\n<g id=\"edge15\" class=\"edge\">\n<title>132699680135904&#45;&gt;132699680135760</title>\n<path fill=\"none\" stroke=\"black\" d=\"M199.25,-286.66C198.77,-269.17 197.68,-229.8 197.02,-206.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"200.52,-206.06 196.74,-196.16 193.52,-206.25 200.52,-206.06\"/>\n</g>\n<!-- 132699680136000&#45;&gt;132699680135904 -->\n<g id=\"edge16\" class=\"edge\">\n<title>132699680136000&#45;&gt;132699680135904</title>\n<path fill=\"none\" stroke=\"black\" d=\"M311.7,-457.47C314.25,-435.82 318.03,-377.91 291.5,-342 280.12,-326.59 261.96,-316.26 244.78,-309.47\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"245.97,-306.18 235.38,-306.05 243.58,-312.76 245.97,-306.18\"/>\n</g>\n<!-- 132699680135568&#45;&gt;132699680135904 -->\n<g id=\"edge18\" class=\"edge\">\n<title>132699680135568&#45;&gt;132699680135904</title>\n<path fill=\"none\" stroke=\"black\" d=\"M199.5,-341.75C199.5,-334.8 199.5,-324.85 199.5,-316.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"203,-316.09 199.5,-306.09 196,-316.09 203,-316.09\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x78b08d4bd970>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 BackPropagation"
      ],
      "metadata": {
        "id": "l7zp3BC0aoFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Backprogation referes to the method of calculating the gradients of neural networks parameters.\n",
        "* This method transverses the network transverses the network in reverse order, from output to the input layer, according the **chain rule** from calculus.\n",
        "* The algorithm stores any intermediate variables required while calculating the gradient with respect to some parameters.\n",
        "* Assume that we have functions $Y=f(x)$ and $Z=g(x)$, in which the input and output X,Y,Z are tensors of random shapes.\n",
        "* By using chain rule,we can compute the derivative of $Z$ with respect to $X$ via:\n",
        "  $\\frac{\\partial Z}{\\partial X} = \\frac{\\partial Z}{\\partial Y} \\frac{\\partial Y}{\\partial X}$\n",
        "\n",
        "* Here we use the prod operator to multiply its arguments after necessary operations.\n",
        "* Recall that the parameters of the simple forward pass we previously attempted,its parameters are $W^{(1)}$ and $W^{(2)}$.\n",
        "* The objective of backpropagation is to calculate the gradients $\\frac{\\partial J}{\\partial W^{(1)}} \\quad \\text{and} \\quad \\frac{\\partial J}{\\partial W^{(2)}}$.\n",
        "* To accomplish this, we apply chain rule and calculate,in turn the gradients of each intermediate variable and parameter.\n",
        "* First we calculate the gradient of the objective function $J=L+s$ with respect to the loss term $L$ and the regularization term $S$:\n",
        "  $\\frac{\\partial J}{\\partial L} = 1 \\quad \\text{and} \\quad \\frac{\\partial J}{\\partial s} = 1$\n",
        "  \n"
      ],
      "metadata": {
        "id": "AX4rlsOWarVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dj_dl =1\n",
        "dj_ds = 1\n"
      ],
      "metadata": {
        "id": "_X1sSuUOfOHM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we compute the gradient of the objective function with respect to  variable of the output layer $o$ according to chain rule:\n",
        "  $\\frac{\\partial J}{\\partial o} = \\frac{\\partial J}{\\partial L} \\frac{\\partial L}{\\partial o} = \\frac{\\partial L}{\\partial o} \\in \\mathbb{R}^q$"
      ],
      "metadata": {
        "id": "3an476JrfZbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = o.numel()"
      ],
      "metadata": {
        "id": "L2sxf8hHnhpf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl_do = ((o-y)*2)/N\n",
        "dl_do"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noBB_XjSf1Jn",
        "outputId": "89e243e5-b77b-4b16-ba38-cd7d42aa963e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2160,  0.2652,  0.3968,  0.3439],\n",
              "        [ 0.0698, -0.0016,  0.1421,  0.0833],\n",
              "        [ 0.3821,  0.3704,  0.2947,  0.1156],\n",
              "        [ 0.3585,  0.4021,  0.4179,  0.1451]], grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dj_do = dj_dl*dl_do\n",
        "dj_do"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60A0iWfFhaa4",
        "outputId": "3ee5dd68-06cb-4f7f-a628-6946b4ef6d42"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2160,  0.2652,  0.3968,  0.3439],\n",
              "        [ 0.0698, -0.0016,  0.1421,  0.0833],\n",
              "        [ 0.3821,  0.3704,  0.2947,  0.1156],\n",
              "        [ 0.3585,  0.4021,  0.4179,  0.1451]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we calculate the gradients of the regularization term with respect to both parameters:  \n",
        "  $\\frac{\\partial s}{\\partial W^{(1)}} = \\lambda W^{(1)} \\quad \\text{and} \\quad \\frac{\\partial s}{\\partial W^{(2)}} = \\lambda W^{(2)}$"
      ],
      "metadata": {
        "id": "h51XnLxzcDxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_dw1 = lambda_*w_1\n",
        "ds_dw2 = lambda_*w_2\n",
        "ds_dw1,ds_dw2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iDgS627iPsT",
        "outputId": "ce7009a0-5b0d-46f7-8772-0338edb722de"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0314, 0.0596, 0.0075],\n",
              "         [0.0277, 0.0951, 0.0640],\n",
              "         [0.0290, 0.0036, 0.0223],\n",
              "         [0.0915, 0.0770, 0.0285]], grad_fn=<MulBackward0>),\n",
              " tensor([[0.0531, 0.0993, 0.0663, 0.0689],\n",
              "         [0.0410, 0.0021, 0.0331, 0.0336],\n",
              "         [0.0760, 0.0832, 0.0446, 0.0658],\n",
              "         [0.0816, 0.0828, 0.0214, 0.0162]], grad_fn=<MulBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now we are able to calculate the gradient $\\frac{\\partial J}{\\partial W^{(2)}} $ of the model parameters closest to the output layer. Using chain rule yields:\n",
        "  $\\frac{\\partial J}{\\partial W^{(2)}} = \\frac{\\partial J}{\\partial o} \\frac{\\partial o}{\\partial W^{(2)}} + \\frac{\\partial J}{\\partial s} \\frac{\\partial s}{\\partial W^{(2)}} = \\frac{\\partial L}{\\partial o} h^\\top + \\lambda W^{(2)}$"
      ],
      "metadata": {
        "id": "sLwjYnqPi5It"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dj_dw2 = dj_do*h.T + lambda_*w_2\n",
        "dj_dw2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHmCpKKobype",
        "outputId": "656a9586-e048-4e44-b89b-1d082747d665"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.2229e-01,  3.1636e-01,  2.3814e-01,  4.0787e-01],\n",
              "        [ 9.0606e-02, -2.6187e-05,  7.7967e-02,  1.3993e-01],\n",
              "        [ 2.6377e-01,  4.2071e-01,  1.1947e-01,  1.7650e-01],\n",
              "        [ 2.4034e-01,  5.2645e-01,  1.6878e-01,  1.5738e-01]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* To obtain the gradients with respect to $W^{(1)}$ we need to continue backpropagation along the output layer to hidden layer.\n",
        "* The gradient with respect to hidden layer output $\\frac{\\partial J}{\\partial h} \\in \\mathbb{R}^h$ is given by.\n",
        "  $\\frac{\\partial J}{\\partial h} = \\frac{\\partial J}{\\partial o} \\frac{\\partial o}{\\partial h} = (W^{(2)})^\\top \\frac{\\partial J}{\\partial o}$\n",
        "\n"
      ],
      "metadata": {
        "id": "Z-_p-KWjlYGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dj_dh = torch.matmul(w_2.T,dj_do)\n",
        "dj_dh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48w8KDvStTGV",
        "outputId": "6b87ae54-7b40-47c7-8c9b-ad25c8ea6718"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7261, 0.7497, 0.8337, 0.4228],\n",
              "        [0.8307, 0.9044, 0.9883, 0.5595],\n",
              "        [0.4138, 0.4269, 0.5314, 0.3384],\n",
              "        [0.4817, 0.4909, 0.5826, 0.3644]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Since the activation function $ùõü$ applies elementwise, calculating th gradient $\\frac{\\partial J}{\\partial z} \\in \\mathbb{R}^h$ of the intermediate variable $z$ requires that we use the elementwise multiplication operator $\\odot$.\n",
        "  $\\frac{\\partial J}{\\partial z} = \\frac{\\partial J}{\\partial h} \\odot \\frac{\\partial h}{\\partial z} = \\frac{\\partial J}{\\partial h} \\odot \\phi'(z)$"
      ],
      "metadata": {
        "id": "W6p_2Jl6tfb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dj_dz = dj_dh*relu(z)\n",
        "dj_dz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSeO_Q46uYDr",
        "outputId": "a119694c-34a8-4654-c037-db080964200a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2328, 0.5331, 0.4096, 0.1873],\n",
              "        [0.6801, 1.2667, 0.9006, 0.6172],\n",
              "        [0.1792, 0.1348, 0.1349, 0.1193],\n",
              "        [0.4749, 0.6263, 0.5577, 0.3547]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Finally we obtain the gradient $\\frac{\\partial J}{\\partial W^{(1)}} \\in \\mathbb{R}^{h \\times d}$ of the model parameters closest to the input layer.\n",
        "* According the chain rule we get:\n",
        "  $\\frac{\\partial J}{\\partial W^{(1)}} = \\frac{\\partial J}{\\partial z} \\frac{\\partial z}{\\partial W^{(1)}} + \\frac{\\partial J}{\\partial s} \\frac{\\partial s}{\\partial W^{(1)}} = \\frac{\\partial J}{\\partial z} x^\\top + \\lambda W^{(1)}$"
      ],
      "metadata": {
        "id": "cmRB7FyUuivj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dj_dw1 = torch.matmul(dj_dz,x.T) + lambda_*w_1\n",
        "dj_dw1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlUaAGQxu-Yr",
        "outputId": "02962524-f25b-450f-a266-f4fb267501bb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7221, 0.8141, 0.9579],\n",
              "        [1.8093, 1.9188, 2.5712],\n",
              "        [0.3436, 0.2402, 0.4565],\n",
              "        [1.1548, 1.0582, 1.4939]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Training Neural Networks"
      ],
      "metadata": {
        "id": "D747nBObwX2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* When training neural networks, forward and backward propagation dependes on each otherr.\n",
        "* For forward propagation, we transverse the computational graph in the direction of dependecies and compute all the variables on its path.\n",
        "* When training neural networks, once model parameters are intialized,we alternate forward propagation with backward propagation, updating model parameters using gradients given by backward propagation."
      ],
      "metadata": {
        "id": "uXBO_nsawdkY"
      }
    }
  ]
}